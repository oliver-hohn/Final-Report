\chapter{Background}
The background should set the project into context by motivating the subject matter and relating it to existing published work. The background will include a critical evaluation of the existing literature in the area in which your project work is based and should lead the reader to understand how your work is motivated by and related to existing work.

//explain what an event (from what is it built off)
The resulting system aims to produce a timeline of events based on the input text. An event is given by its date(s), subjects and a short summary of the sentence that produced it. An event is produced when a given sentence contains a date. An event can have more than 1 date if it is considered to happen in a range of dates. For example, an event that happened in the 1980s would have two dates, one for the start date: 1980-01-01, and one for the end date: 1989-12-31. While an event that happened just on one day would have just one date. The subjects of an event are given by the "person, place, thing, or idea that is doing or being something"\cite{grammar}.

\section{Natural Language Processing}
-explain what NLP is
\par The projects primary area of research is Natural Language Processing (NLP). NLP is the area of computer science where the aim is to translate human readable and spoken language to a computer (cite). This requires the human input to be subject to constraints. Thereby in this project, expections on the input text are assumed. For example, it is to be expected that documents are written in correct English. As every language has their own grammar, and thereby, their own rules, to expand the system to different languages would require different rule sets to be applied depending on the language to process the text. This includes the algorithms used in the summary of sentences used in events (discussed in a latter section).
\par A relevant issue is the input documents having text that is disorganised in a grammatical sense. Many NLP software tools (incluiding StanfordCoreNLP used in this project) perform extremely poorly with such input text. For example, in the paper Named Entity Recognition in Tweets: An Experimental Study \cite{ritter2011}, where they looked at the performance of popular NLP tools on "Tweets", which due to them being limited to a character count will use abbrevations that do not make sense grammatically. This is due to the fact that the NLP tools and algorithms cannot apply their rules and models to the text to identify the different components. Thereby, in this project the assumption is that the input will be in correct English, as the systems primary user is a law professional. NLP will be presented in more detail in the following section.
-what parts of NLP are involved
\par NLP is a broad area of study. However, in this project the focus is on Automatic Summarization, Named-Entity Recognition (NER), and Sentence Breaking.
\par In Automatic Summarization the aim is to produce a shorter version (the summary) of a given input text, that still holds the same meaning of the original input. The summary can be built directly from the words in the input, or it can built using a dictionary. As in this project, an event is built from one sentence that contains a date, the specific area of Automatic Summarization which was focused on was Headline Generation. This is where a summary is built based on a given input text, such that the summary falls below a certain threshold value. For Headline Generation there are two main implementations: statistic based and decision (or trimming) based \cite{daumemarcu2002}.
\par In the statistic based model, where Noisy-Channel models are the most prominent, as shown by the multitude of publications \cite{daumemarcu2002,rushchopraweston2015,chopraaulirush2016}. In noisy-channel models, the belief is that the summary of the given input lies within the text but it is surrounded by unwanted noise (text). These systems require a large collection of annotated data (pairs of input and their summary), which is used in the calculation of the statistics of whether or not a produced summary correctly represents the input. Examples of these algorithms can be found in the works of \cite{daumemarcu2002,knightmarcu2000}.
\par The decision based models, are older than the statistic based models and use the grammar of the input text to trim (remove) parts of the inputs until no more rules can be applied or the summary produced is below a given threshold \cite{dorrzajicschwartz2003}. This is done by tokenizing, breaking an input text into words, phrases, symbols and tagging them each by an identifier(cite). The tokenized text, which is usually represented as a tree where the leaves are the words in the input text and the inner children the identifiers, is passed through an algorithm which applies rules which removes branches of the tree until no more rules can be applied or the summary text is below a given threshold. The trimmed tree is then used to produce the summary.
//compare statistics to decision
\par The trimming based models do not tend to produce as good of summaries as the statistic based model, due to them producing, usually, only one summary while the statistic based models produce a selection to choose from. However, as can be seen from the works of Knight and Marcu \cite{knightmarcu2000}, the trimming based models can produce better summaries than the statistic models in some occassions. The main advantage of the trimming model is their speed, and not requiring a large corpus of data (like the statistic models) by relying on the grammar to build the summary. In newer works of text summarization, neural network models are being used. These fall under the statistical based models. They produce extremely accurate results, but as most statistical models they require a large corpus of data. They requirement of the extensive sample of annotated data also leads to these algorithms being processor-heavy, as the data needs to be read, and calculations need to be performed to produce the probability values used in identifying suitable summaries.
\par Due to the time-constraints of the project, it was decided to use the trimming approach, in specific the algorithm provided by Dorr, Zajic, and Schwartz \cite{dorrzajicschwartz2003} (figure of algorthim). Where the given input text is turned into a tree based on its grammatical structure, with the words in the text as the leaves, and the inner nodes being the identifiers, which is then trimmed. In addition, in statistical models the input text size is multiple sentences, i.e. a paragraph or more of text, where loading the models from the annotated data has less of an impact in performance as this being done for every sentence in that input text. 
-cite
\begin{algorithm}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
	\Input{A text with events to be found}
	\Output{A list of Events (Results)}
	\For{every sentence in the input text}
		create an event\;
		use NER annotator to add subjects (LOCATION, PERSON, ORGANIZATION, MONEY, MISC) and parse the DATEs\;
		\If{found a date(s)}{
			produce a summary of the sentence and set it for the event \ref{alg:trim}\;
			add the event to the list of Events\;
		}
		return the list of Events\;	
\label{alg:getEvent}
\caption{Get an Event}
\end{algorithm}
\begin{algorithm}
\SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
	\Input{A Grammatical Tree T of the Sentence to summarize}
	\Input{threshold: Threshold value}
	\Output{A Summary of the given sentence}
	get the leftmost-lower subtree with root S\;
	remove time expressions\;
	remove determiners (e.g. 'a', 'the')\;
	\While{the number of leaves in tree $>$ threshold and there are subtrees to remove with this rule}{
		remove all the children except the first, where the rightmost-lowest subtree with root XP and its first child also being an identifier XP (where XP can be NP, VP, or S)\;
	}
	\While{the number of leaves in tree $>$ threshold and there are subtrees to remove with this rule}{
		remove any XP (where XP can be NP,VP,PP) before the first NP found\;
	}
	get Tree T' from lastRule\;
	from T' create a sentence S by reading of the leaves in pre-order\;
	return S\;
\caption{Dorr, B., Zajic, D. and Schwartzm R, (2003). Hedge Trimmer: A Parse-and-Trim Approach to Headline Generation}
\label{alg:trim}
\end{algorithm}
\begin{algorithm}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{A Grammatical Tree T of the Sentence to summarize}
\Input{threshold: Threshold value}
\Output{A Grammatical Tree of the Summary of the input after the last rule has been applied}
\If{the number of leaves in tree > threshold}{
		make a copy of the tree T'\;
		\While{the number of leaves in tree $>$ threshold and there are subtrees to remove with this rule}{
			remove any trailling PP nodes (and their children)\;
		}
		\If{the number of leaves in tree $>$ threshold}{
			\While{the number of leaves in tree $>$ threshold and there are subtrees to remove with this rule}{
				remove any trailling SBARs (and their children)\;
			}
			\While{the number of leaves in tree $>$ threshold and there are subtrees to remove with this rule}{
				remove any trailling PPs (and their children)\;
			}
		}{
			return T'\;
		}
}
return T\;
\label{alg:lastRule}
\caption{Last Rule}
\end{algorithm}
\begin{algorithm}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Base Date}
\Input{NER DATE}
\Output{date1 being the lowest Date, and date2 the greatest Date}
	List L\;
	split the date by "/"\;
	\ForEach{date in the split}{
		call getDate() on the date\;
		add result to list L\;
	}
	call enforceRule()\;
\label{alg:parse}
\caption{Parse NER Dates}
\end{algorithm}

\begin{algorithm}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{A Normalized Date d of the format given by ISO 8601}
\Input{Base Date}
\Output{Exact Dates in a List L}
initialize year1,month1,day1 to their default values\;
set year2,month2,day2 to null\;
\If{d matches PAST REF pattern}{
	create a date1 with format 0001-01-01\;
	create a date2 with value of the Base Date\;
	add date1 and date2 to L\;
}
\If{d matches PRESENT REF pattern}{
	create a date1 with value of the Base Date\;
	add date1 to L\;
}
\If{d matches FUTURE REF pattern}{
	create a date1 with value of the Base Date\;
	create a date2 with format 9999-12-31\;
	add date1 and date2 to L\;
}{
	check if its a BC date\;
	split the input by '-'\;
	\For{i =0; i $<$ split.length; i{+}{+}}{
		\If{i == 0}{
			\If{split[i] matches year pattern}{
				set year1 to it, replacing every X with a 0\;
				set year2 to it, replacing every X with a 9\;
			}
		}
		\If{i == 1}{
			\If{split[i] matches month pattern}{
				set month1 to it\;
			}
			\If{split[i] matches week pattern}{
				calculate month1, day1. month2, and day2 using the week data\;
			}
			\If{split[i] matches season pattern}{
				calculate month1,day1,year2,month2,day2 using season data\;
			}
		}
		\If{i == 2}{
			\If{we found a week pattern before and split[i] is a decimal value}{
				calculate which day of the week it is\;
				set year1,month1,day1 using the week and day of the week data\;
			}
			\If{split[i] matches day pattern}{
				set day1 to it\;
			}
			\If{split[i] matches weekend pattern}{
				set day1, year2, month2, day2 using weekedn data\;
			}
		}
	}
	create a Date for date1 using year1,month1,day1 data\;
	add date1 to L\;
	\If{year2,month2,and day2 have values}{
		create a Date for date2 using year2,month2,day2 data\;
		add date2 to L\;
	}
}
return L\;
\label{alg:getDate}
\caption{getDate}
\end{algorithm}

\begin{algorithm}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{List of new Dates L}
\Output{date1 being the lowest Date up to now}
\Output{date2 being the greatest Date up to now}
\ForEach{Date d in the list L}{
	\If{date1 $>$ d}{
		date1 $:=$ d\;
	}
	\If{(date2 is null $||$ date2 $<$ d) and $date1 \neq d$}{
		date2 $:=$ d\;
	}
}
\label{alg:enforceRule}
\caption{Enforce Rule}
\end{algorithm}
\begin{figure}
\resizebox{\linewidth}{!}{
\Tree
[.ROOT
	[.S 
		[.PP 
			[.IN \textit{On} ]
               		[.NP 
				[.NNP \textit{Friday} ]
			]
		]
          	[.NP 
			[.DT \textit{the} ]
			[.NNP \textit{Washington} ]
			[.NNP \textit{Post} ]
		]
		[.VP 
			[.VBD \textit{came} ]
			[.PRT 
				[.RP \textit{out} ]
			]
			[.PP 
				[.IN \textit{with} ]
				[.NP 
					[.NP 
						[.DT \textit{the} ] 
						[.JJS \textit{latest} ]
					]
					[.PP 
						[.IN \textit{from} ] 
						[.NP 
							[.PRP \textit{its} ] 
							[.JJ \textit{long-running} ] 
							[.NN \textit{investigation} ]
						]
					]
				]
			]
			[.PP 
				[.IN \textit{into} ]
				[.NP
					[.NP 
						[.NNP\textit{Trump} ]
						[.POS \textit{'s} ]
					]
					[.JJ \textit{charitable} ]
					[.JJ \textit{donations} ]
				]
			]
		]
		[.DOT \textit{.} ]
	]
]
}
\caption{P.O.S/Grammatical Tree of: "On Friday the Washington Post came out with the latest from its long-running investigation into Trump's charitable donations."}
\label{tree:origString}
\end{figure}
\begin{figure}
\resizebox{\linewidth}{!}{
\Tree
[.ROOT
	[.S 
          	[.NP 
			[.DT \textit{the} ]
			[.NNP \textit{Washington} ]
			[.NNP \textit{Post} ]
		]
		[.VP 
			[.VBD \textit{came} ]
			[.PRT 
				[.RP \textit{out} ]
			]
			[.PP 
				[.IN \textit{with} ]
				[.NP 
					[.NP 
						[.DT \textit{the} ] 
						[.JJS \textit{latest} ]
					]
					[.PP 
						[.IN \textit{from} ] 
						[.NP 
							[.PRP \textit{its} ] 
							[.JJ \textit{long-running} ] 
							[.NN \textit{investigation} ]
						]
					]
				]
			]
			[.PP 
				[.IN \textit{into} ]
				[.NP
					[.NP 
						[.NNP\textit{Trump} ]
						[.POS \textit{'s} ]
					]
					[.JJ \textit{charitable} ]
					[.JJ \textit{donations} ]
				]
			]
		]
		[.DOT \textit{.} ]
	]
]
}
\caption{P.O.S/Grammatical Tree after removing time expressions}
\label{tree:removeTime}
\end{figure}
\begin{figure}
\resizebox{\linewidth}{!}{
\Tree
[.ROOT
	[.S 
          	[.NP 
			[.NNP \textit{Washington} ]
			[.NNP \textit{Post} ]
		]
		[.VP 
			[.VBD \textit{came} ]
			[.PRT 
				[.RP \textit{out} ]
			]
			[.PP 
				[.IN \textit{with} ]
				[.NP 
					[.NP 
						[.JJS \textit{latest} ]
					]
					[.PP 
						[.IN \textit{from} ] 
						[.NP 
							[.PRP \textit{its} ] 
							[.JJ \textit{long-running} ] 
							[.NN \textit{investigation} ]
						]
					]
				]
			]
			[.PP 
				[.IN \textit{into} ]
				[.NP
					[.NP 
						[.NNP\textit{Trump} ]
						[.POS \textit{'s} ]
					]
					[.JJ \textit{charitable} ]
					[.JJ \textit{donations} ]
				]
			]
		]
		[.DOT \textit{.} ]
	]
]
}
\caption{P.O.S/Grammatical Tree after removing determiners}
\label{tree:removeDeterminers}
\end{figure}
\begin{figure}
\resizebox{\linewidth}{!}{
\Tree
[.ROOT
	[.S 
          	[.NP 
			[.NNP \textit{Washington} ]
			[.NNP \textit{Post} ]
		]
		[.VP 
			[.VBD \textit{came} ]
			[.PRT 
				[.RP \textit{out} ]
			]
			[.PP 
				[.IN \textit{with} ]
				[.NP 
					[.NP 
						[.JJS \textit{latest} ]
					]
					[.PP 
						[.IN \textit{from} ] 
						[.NP 
							[.PRP \textit{its} ] 
							[.JJ \textit{long-running} ] 
							[.NN \textit{investigation} ]
						]
					]
				]
			]
			[.PP 
				[.IN \textit{into} ]
				[.NP
					[.NP 
						[.NNP\textit{Trump} ]
						[.POS \textit{'s} ]
					]
				]
			]
		]
		[.DOT \textit{.} ]
	]
]
}
\caption{P.O.S/Grammatical Tree after XP Over-XP first iteration}
\label{tree:xpOverXp}
\end{figure}
\begin{figure}
\resizebox{\linewidth}{!}{
\Tree
[.ROOT
	[.S 
          	[.NP 
			[.NNP \textit{Washington} ]
			[.NNP \textit{Post} ]
		]
		[.VP 
			[.VBD \textit{came} ]
			[.PRT 
				[.RP \textit{out} ]
			]
			[.PP 
				[.IN \textit{with} ]
				[.NP 
					[.NP 
						[.JJS \textit{latest} ]
					]
				]
			]
			[.PP 
				[.IN \textit{into} ]
				[.NP
					[.NP 
						[.NNP\textit{Trump} ]
						[.POS \textit{'s} ]
					]
				]
			]
		]
		[.DOT \textit{.} ]
	]
]
}
\caption{P.O.S/Grammatical Tree after XP-Over-XP second iteration}
\end{figure}
\section{Data Processing and Representation}
\par In the grammatical tree formed, the inner leave identifiers are given by the P.O.S Treebank (cite). Each word or set of words are given a part of speech tag identifying them. An example can be found below. (example figure).
\par On the grammatical tree shown in the previous figure, we will apply the algorithm proposed by Dorr, Zajic, and Schwartz \cite{dorrzajicschwartz2003}. This can be shown graphically in the following figure. It is to be noted that the value of the threshold does not force the summary to be below it, but it is a length of the summary to which the algorithm is working to.
-tags are POS (cite)
-example of input text to tree
-example of producing summary
-algorithm
-what are the options for the summary (Neural Networks vs Decision-Based)
-give an algorithm for determining the summary, with an example
-explain the date problem, with example (determine that it uses an ISO standard)



//defintion of an event

