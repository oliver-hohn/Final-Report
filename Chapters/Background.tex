\chapter{Background}
The background should set the project into context by motivating the subject matter and relating it to existing published work. The background will include a critical evaluation of the existing literature in the area in which your project work is based and should lead the reader to understand how your work is motivated by and related to existing work.

//explain what an event (from what is it built off)
The resulting system aims to produce a timeline of events based on the input text. An event is given by its date(s), subjects and a short summary of the sentence that produced it. An event is produced when a given sentence contains a date. An event can have more than 1 date if it is considered to happen in a range of dates. For example, an event that happened in the 1980s would have two dates, one for the start date: 1980-01-01, and one for the end date: 1989-12-31. While an event that happened just on one day would have just one date. The subjects of an event are given by the "person, place, thing, or idea that is doing or being something"\cite{grammar}.

\section{Natural Language Processing}
-explain what NLP is
\par The projects primary area of research is Natural Language Processing (NLP). NLP is the area of computer science where the aim is to translate human readable and spoken language to a computer (cite). This requires the human input to be subject to constraints. Thereby in this project, expections on the input text are assumed. For example, it is to be expected that documents are written in correct English. As every language has their own grammar, and thereby, their own rules, to expand the system to different languages would require different rule sets to be applied depending on the language to process the text. This includes the algorithms used in the summary of sentences used in events (discussed in a latter section).
\par A relevant issue is the input documents having text that is disorganised in a grammatical sense. Many NLP software tools (incluiding StanfordCoreNLP used in this project) perform extremely poorly with such input text. For example, in the paper Named Entity Recognition in Tweets: An Experimental Study \cite{ritter2011}, where they looked at the performance of popular NLP tools on "Tweets", which due to them being limited to a character count will use abbrevations that do not make sense grammatically. This is due to the fact that the NLP tools and algorithms cannot apply their rules and models to the text to identify the different components. Thereby, in this project the assumption is that the input will be in correct English, as the systems primary user is a law professional. NLP will be presented in more detail in the following section.
-what parts of NLP are involved
\par NLP is a broad area of study. However, in this project the focus is on Automatic Summarization, Named-Entity Recognition (NER), and Sentence Breaking.
\par In Automatic Summarization the aim is to produce a shorter version (the summary) of a given input text, that still holds the same meaning of the original input. The summary can be built directly from the words in the input, or it can built using a dictionary. As in this project, an event is built from one sentence that contains a date, the specific area of Automatic Summarization which was focused on was Headline Generation. This is where a summary is built based on a given input text, such that the summary falls below a certain threshold value. For Headline Generation there are two main implementations: statistic based and decision (or trimming) based \cite{daumemarcu2002}.
\par In the statistic based model, where Noisy-Channel models are the most prominent, as shown by the multitude of publications \cite{daumemarcu2002,rushchopraweston2015,chopraaulirush2016}. In noisy-channel models, the belief is that the summary of the given input lies within the text but it is surrounded by unwanted noise (text). These systems require a large collection of annotated data (pairs of input and their summary), which is used in the calculation of the statistics of whether or not a produced summary correctly represents the input. Examples of these algorithms can be found in the works of \cite{daumemarcu2002,knightmarcu2000}.
\par The decision based models, are older than the statistic based models and use the grammar of the input text to trim (remove) parts of the inputs until no more rules can be applied or the summary produced is below a given threshold \cite{dorrzajicschwartz2003}. This is done by tokenizing, breaking an input text into words, phrases, symbols and tagging them each by an identifier(cite). The tokenized text, which is usually represented as a tree where the leaves are the words in the input text and the inner children the identifiers, is passed through an algorithm which applies rules which removes branches of the tree until no more rules can be applied or the summary text is below a given threshold. The trimmed tree is then used to produce the summary.
//compare statistics to decision
\par The trimming based models do not tend to produce as good of summaries as the statistic based model, due to them producing, usually, only one summary while the statistic based models produce a selection to choose from. However, as can be seen from the works of Knight and Marcu \cite{knightmarcu2000}, the trimming based models can produce better summaries than the statistic models in some occassions. The main advantage of the trimming model is their speed, and not requiring a large corpus of data (like the statistic models) by relying on the grammar to build the summary. In newer works of text summarization, neural network models are being used. These fall under the statistical based models. They produce extremely accurate results, but as most statistical models they require a large corpus of data. They requirement of the extensive sample of annotated data also leads to these algorithms being processor-heavy, as the data needs to be read, and calculations need to be performed to produce the probability values used in identifying suitable summaries.
\par Due to the time-constraints of the project, it was decided to use the trimming approach, in specific the algorithm provided by Dorr, Zajic, and Schwartz \cite{dorrzajicschwartz2003} (Figures \ref{alg:trim} and \ref{alg:last}). Where the given input text is turned into a tree based on its grammatical structure, with the words in the text as the leaves, and the inner nodes being the identifiers, which is then trimmed. In addition, in statistical models the input text size is multiple sentences, i.e. a paragraph or more of text, where loading the models from the annotated data has less of an impact in performance as this being done for every sentence in that input text. 
-cite

\begin{algorithm}[H]
\SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
	\Input{A Grammatical Tree T of the Sentence to summarize}
	\Input{threshold: Threshold value}
	\Output{A Summary of the given sentence}
	get the leftmost-lower subtree with root S\;
	remove time expressions\;
	remove determiners (e.g. 'a', 'the')\;
	\While{the number of leaves in tree $>$ threshold and there are subtrees to remove with this rule}{
		remove all the children except the first, where the rightmost-lowest subtree with root XP and its first child also being an identifier XP (where XP can be NP, VP, or S)\;
	}
	\While{the number of leaves in tree $>$ threshold and there are subtrees to remove with this rule}{
		remove any XP (where XP can be NP,VP,PP) before the first NP found\;
	}
	get Tree T' from lastRule\;
	from T' create a sentence S by reading of the leaves in pre-order\;
	return S\;
\caption{Dorr, B., Zajic, D. and Schwartzm R, (2003). Hedge Trimmer: A Parse-and-Trim Approach to Headline Generation}
\label{alg:trim}
\end{algorithm}
\begin{algorithm}[H]
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{A Grammatical Tree T of the Sentence to summarize}
\Input{threshold: Threshold value}
\Output{A Grammatical Tree of the Summary of the input after the last rule has been applied}
\If{the number of leaves in tree > threshold}{
		make a copy of the tree T'\;
		\While{the number of leaves in tree $>$ threshold and there are subtrees to remove with this rule}{
			remove any trailling PP nodes (and their children)\;
		}
		\If{the number of leaves in tree $>$ threshold}{
			\While{the number of leaves in tree $>$ threshold and there are subtrees to remove with this rule}{
				remove any trailling SBARs (and their children)\;
			}
			\While{the number of leaves in tree $>$ threshold and there are subtrees to remove with this rule}{
				remove any trailling PPs (and their children)\;
			}
		}{
			return T'\;
		}
}
return T\;
\caption{Last Rule}
\label{alg:last}
\end{algorithm}

\section{Data Processing and Representation}
\par In the grammatical tree formed, the inner leave identifiers are given by the P.O.S Treebank\footnote{\url{http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html}}. Each word or set of words are given a part of speech tag identifying them. An example can be found below. (example figure).
\par On the grammatical tree shown in the previous figure, we will apply the algorithm proposed by Dorr, Zajic, and Schwartz \cite{dorrzajicschwartz2003}. This can be shown graphically in the following figure. It is to be noted that the value of the threshold does not force the summary to be below it, but it is a length of the summary to which the algorithm is working to. For the sentence: "On Friday the Washington Post came out with the latest from its long-running investigation into Trump's charitable donations.", the following gramamtical tree is produced \ref{tree:origString}.
\begin{figure}[h]
\resizebox{\linewidth}{!}{
\Tree
[.ROOT
	[.S 
		[.PP 
			[.IN \textit{On} ]
               		[.NP 
				[.NNP \textit{Friday} ]
			]
		]
          	[.NP 
			[.DT \textit{the} ]
			[.NNP \textit{Washington} ]
			[.NNP \textit{Post} ]
		]
		[.VP 
			[.VBD \textit{came} ]
			[.PRT 
				[.RP \textit{out} ]
			]
			[.PP 
				[.IN \textit{with} ]
				[.NP 
					[.NP 
						[.DT \textit{the} ] 
						[.JJS \textit{latest} ]
					]
					[.PP 
						[.IN \textit{from} ] 
						[.NP 
							[.PRP \textit{its} ] 
							[.JJ \textit{long-running} ] 
							[.NN \textit{investigation} ]
						]
					]
				]
			]
			[.PP 
				[.IN \textit{into} ]
				[.NP
					[.NP 
						[.NNP\textit{Trump} ]
						[.POS \textit{'s} ]
					]
					[.JJ \textit{charitable} ]
					[.JJ \textit{donations} ]
				]
			]
		]
		[.DOT \textit{.} ]
	]
]
}
\caption{P.O.S/Grammatical Tree of: "On Friday the Washington Post came out with the latest from its long-running investigation into Trump's charitable donations."}
\label{tree:origString}
\end{figure}
\par The inner nodes in the tree are identifiers given by the P.O.S Treebank, and the leaves are the words in the sentence. A parent identifier can be a broader identifier of a collection of sub-identifiers. Using the algorithm described previously \cite{dorrzajicschwartz2003}, the tree can be processed as follows. The following example is used as a visualisation of how the decision-based algorithm works. Firstly, the lowest-leftmost S must be identified, as can be seen from the figure \ref{tree:origString} there is only one subtree with root S. This subtree is  extracted, and the algorithm is continued on it. The next step is to remove time expressions, of which there is only one "Friday", however we must remove its parent to (and thereby its children) to avoid producing a grammatically incorrect summary. The result is graphically shown in the following figure \ref{tree:removeTime}.
\begin{figure}[h]
\resizebox{\linewidth}{!}{
\Tree
	[.S 
          	[.NP 
			[.DT \textit{the} ]
			[.NNP \textit{Washington} ]
			[.NNP \textit{Post} ]
		]
		[.VP 
			[.VBD \textit{came} ]
			[.PRT 
				[.RP \textit{out} ]
			]
			[.PP 
				[.IN \textit{with} ]
				[.NP 
					[.NP 
						[.DT \textit{the} ] 
						[.JJS \textit{latest} ]
					]
					[.PP 
						[.IN \textit{from} ] 
						[.NP 
							[.PRP \textit{its} ] 
							[.JJ \textit{long-running} ] 
							[.NN \textit{investigation} ]
						]
					]
				]
			]
			[.PP 
				[.IN \textit{into} ]
				[.NP
					[.NP 
						[.NNP\textit{Trump} ]
						[.POS \textit{'s} ]
					]
					[.JJ \textit{charitable} ]
					[.JJ \textit{donations} ]
				]
			]
		]
		[.DOT \textit{.} ]
	]
}
\caption{P.O.S/Grammatical Tree after removing time expressions}
\label{tree:removeTime}
\end{figure}
\par On the resulting tree, the algorithm dictates the removal of some determiners. A determiner is identified by the "DT" parent tag, however not all of the parents identified are removed, only ones that have a child of label "the" or "a". The resulting tree of applying this rule is given by Figure \ref{tree:removeDeterminers}.
\begin{figure}[h]
\resizebox{\linewidth}{!}{
\Tree
	[.S 
          	[.NP 
			[.NNP \textit{Washington} ]
			[.NNP \textit{Post} ]
		]
		[.VP 
			[.VBD \textit{came} ]
			[.PRT 
				[.RP \textit{out} ]
			]
			[.PP 
				[.IN \textit{with} ]
				[.NP 
					[.NP 
						[.JJS \textit{latest} ]
					]
					[.PP 
						[.IN \textit{from} ] 
						[.NP 
							[.PRP \textit{its} ] 
							[.JJ \textit{long-running} ] 
							[.NN \textit{investigation} ]
						]
					]
				]
			]
			[.PP 
				[.IN \textit{into} ]
				[.NP
					[.NP 
						[.NNP\textit{Trump} ]
						[.POS \textit{'s} ]
					]
					[.JJ \textit{charitable} ]
					[.JJ \textit{donations} ]
				]
			]
		]
		[.DOT \textit{.} ]
	]
}
\caption{P.O.S/Grammatical Tree after removing determiners}
\label{tree:removeDeterminers}
\end{figure}
\par The threshold value is used in the next following rules. For this example, the threshold value is 10. From the two rules that are left, only one will be applied as the resulting tree's number of leaves will fall below the threshold value, such that the last rule will not be applied to avoid the algorithm from trimming the summary to much. Hence, the threshold's value importance is of stopping the algorithm from over-trimming or under-trimming. When the tree is over-trimmed there is the possibility that the resulting summary does not have the core meaning of the original sentence, or has no meaning. The advantage of under-trimming is that the meaning of the sentence is very likely kept, due to the summary sentence being of greater length of the optimal summary, thereby having more words and such more meaning, and being closer to the original sentence. However, the aim is to produce a summary, i.e. a short sentence with the same meaning as the original, thus under-trimming can ensure the meaning is kept in the summary, but not that the resulting summary is optimal in size or time (when reading many summaries, shorter ones will be appreciated over longer ones). When applying the XP-Over-XP rule, when a parent of identifier type XP (where XP can be NP,VP or S), has a first child identifier of type XP also, then all other children of the parent are removed. This is done iteratively until the resulting tree is below the threshold, or the rule cannot be applied further due to there not being anymore parent XP, first child XP pairs. The first iteration of the rule is shown in Figure \ref{tree:xpOverXpFirst}, and the second and final iteration in Figure \ref{tree:xpOverXpSecond}.
\begin{figure}[h]
\resizebox{\linewidth}{!}{
\Tree
	[.S 
          	[.NP 
			[.NNP \textit{Washington} ]
			[.NNP \textit{Post} ]
		]
		[.VP 
			[.VBD \textit{came} ]
			[.PRT 
				[.RP \textit{out} ]
			]
			[.PP 
				[.IN \textit{with} ]
				[.NP 
					[.NP 
						[.JJS \textit{latest} ]
					]
					[.PP 
						[.IN \textit{from} ] 
						[.NP 
							[.PRP \textit{its} ] 
							[.JJ \textit{long-running} ] 
							[.NN \textit{investigation} ]
						]
					]
				]
			]
			[.PP 
				[.IN \textit{into} ]
				[.NP
					[.NP 
						[.NNP\textit{Trump} ]
						[.POS \textit{'s} ]
					]
				]
			]
		]
		[.DOT \textit{.} ]
	]
}
\caption{P.O.S/Grammatical Tree after XP-Over-XP first iteration}
\label{tree:xpOverXpFirst}
\end{figure}
\begin{figure}[h]
\resizebox{\linewidth}{!}{
\Tree
	[.S 
          	[.NP 
			[.NNP \textit{Washington} ]
			[.NNP \textit{Post} ]
		]
		[.VP 
			[.VBD \textit{came} ]
			[.PRT 
				[.RP \textit{out} ]
			]
			[.PP 
				[.IN \textit{with} ]
				[.NP 
					[.NP 
						[.JJS \textit{latest} ]
					]
				]
			]
			[.PP 
				[.IN \textit{into} ]
				[.NP
					[.NP 
						[.NNP\textit{Trump} ]
						[.POS \textit{'s} ]
					]
				]
			]
		]
		[.DOT \textit{.} ]
	]
}
\caption{P.O.S/Grammatical Tree after XP-Over-XP second iteration}
\label{tree:xpOverXpSecond}
\end{figure}
\par If the number of leaves did not fall under the threshold value, then the next rule to be applied is XP-Before-NP. In this rule, any XP before the NP of the sentence (the grammatical subject) is removed. This would be carried out until the number of leaves falls below the threshold, or the rule cannot be applied further. Finally, the last rule is applied where iteratively trailing PP and SBAR subtrees are removed until the threshold is reached or the rule cant be applied. The result of applying this algorithm (ref algorithm) on the input text: "On Friday the Washington Post came out with the latest from its long-running investigation into Trump's charitable donations.", is "Washington Post came out with latest into Trump's". It should be noted that the original meaning of the sentence, which is that the Washington Post released a new article, is kept. However, it should also be noted that the resulting summary is not grammatically correct as "Trump's" should be "Trump". Even though the summary is not grammatically correct the summary is significantly shorter than the input text, and does represent the same event. It is a headline of the input, and should indicate to a reader what the event is about, to allow them to have a general understanding of what occurred.
-tags are POS (cite) //d
-example of input text to tree //d
-example of producing summary //d 
-algorithm //d 
-what are the options for the summary (Neural Networks vs Decision-Based) //d
-give an algorithm for determining the summary, with an example //d

-explain the date problem, with example (determine that it uses an ISO standard)



//defintion of an event

