\chapter{Background}
\par The background should set the project into context by motivating the subject matter and relating it to existing published work. The background will include a critical evaluation of the existing literature in the area in which your project work is based and should lead the reader to understand how your work is motivated by and related to existing work.

//explain what an event (from what is it built off) //defintion of an event
\par The resulting system should aim to produce a timeline of events based on the input text. An event is given by its date(s), subjects and a short summary of the text that produced it. 
\par For this system, the text of documents will be split into their sentences, which are analyzed separately. An event is produced when a given sentence contains a date (or temporal expression). Events may contain a range of dates. A range is given by a start and end date. The range describes that the event occurred during that time period. For example, an event that occurred in the 1980s would have two dates, one for the start date: 1980-01-01, and one for the end date: 1989-12-31. While an event that happened on one specific day would only have one date associated with it. 
\par The subjects of an event are given by the "person, place, thing, or idea that is doing or being something"\cite{grammar}. These are key words that help convey the meaning of an event. They aid the summary of the event, i.e. summary of the sentence that was identified as this event, by providing additional information used to understand what happened.

\section{Natural Language Processing}
-explain what NLP is
\par The projects primarily enters the field of Natural Language Processing (NLP). NLP is the field in Computer Science that focuses on producing systems that understand human readable and spoken language (cite). While there is ground breaking research in the field, many of the systems require the human input to be subject to constraints. As many of the systems use rule sets that are applied to the input to translate it to a useful input for the system. For example, removing ambiguities in text. 
\par For this system it is expected that the documents are written in correct English. This is because every language has their own grammar. The grammar can be used as a rule set for the language. Thereby using them to identify key words such as people, locations, and companies; and also using them to produce summaries in some case \cite{dorrzajicschwartz2003}. For example, it is to be expected that documents are written in correct English. To expand the system to different languages would require different rule sets to be applied depending on the language to process the text. Different languages would require different algorithms, models and rule sets used.
\par An issue that may occur in the input documents is that they are grammatically disorganised and incorrect. Many NLP software tools (incluiding StanfordCoreNLP which is used in this project) perform extremely poorly with such input. One example is the performance of NLP tools with Tweets, short sentences used in the social media platform Twitter \footnote{\url{https://twitter.com/}}, that use informal abbrevations due to a character limit. In the paper Named Entity Recognition in Tweets: An Experimental Study \cite{ritter2011}, they looked at the performance of popular NLP tools on "Tweets". Many of the different NLP tools such as Apache OpenNLP and Stanford CoreNLP performed poorly. This is due to the NLP tools and their algorithms not being able to apply their rules on to the text to identify its different components. Hence, for this project it will be assumed that the input documents will be written in correct English. This is to be expected as the primary use of this tool is for formal documents, such as law documents. In the future this can be further expanded to other languages.
-what parts of NLP are involved
\par NLP is a broad area of study. For this project the focus is on Automatic Summarization, Named-Entity Recognition (NER), and Sentence Breaking.

\subsection{Automatic Summarization}
\par In Automatic Summarization the aim is to produce a shorter version (a summary) of a given input text. The summary should still hold the same meaning of the original input. The summary can be built directly from the words in the input, or it can built using a dictionary. Since in this project an event is built from a sentence that contains a date, the Headline Generation area of Aumatic Summarization is focused on. In Headline Generation, a summary is built based on the data provided in the input text (its grammar), where a threshold value is given such that the aim is to provide a summary of that size. For Headline Generation there are two main implementations: statistic based and decision (trimming) based \cite{daumemarcu2002}.
\par In the statistic based model, Noisy-Channel models are the most prominent, as shown by the multitude of publications \cite{daumemarcu2002,rushchopraweston2015,chopraaulirush2016}. In noisy-channel models, the belief is that the summary of the given input lies within the text but it is surrounded by unwanted noise (text). These systems require a large collection of annotated data (pairs of input text and their summary), which are used in the calculation of the statistical values used to determine whether a produced summary correctly represents its original text. Examples of these algorithms can be found in the works of \cite{daumemarcu2002,knightmarcu2000}.
\par The decision based models, are older than the statistic based models and use the grammar of the input text to trim (remove) parts of the inputs until no more rules can be applied or the summary produced is below a given threshold \cite{dorrzajicschwartz2003}. This is done by tokenizing, breaking an input text into words, phrases, symbols and tagging them each by an identifier(cite). The tokenized text, which is usually represented as a tree where the leaves are the words in the input text and the inner children the identifiers, is passed through an algorithm which applies rules. These rules remove branches of the tree until no more rules can be applied or the summary text falls below a given threshold. The leaves of the trimmed tree is then used to produce the summary.
//compare statistics to decision
\par The trimming based models do not tend to produce as good of summaries as the statistic based model, due to them producing, usually, only one summary while the statistic based models produce a selection to choose from. However, as can be seen from the works of Knight and Marcu \cite{knightmarcu2000}, the trimming based models can produce better summaries than the statistic models in some occassions. For example, stastic based models are domain dependent as their data set is for a specific domain. Thereby placing statistic models in another domain will cause them to perform poorly compared to the decision models that are domain-independent. 
\par The main advantage of the trimming model is their speed, and not requiring a large corpus of data (like the statistic models) by relying on the grammar to build the summary. In newer works of text summarization, neural network models are used. These fall under the statistical based models. They produce extremely accurate results, but as most statistical models they require a large corpus of data. Requiring a large data set causes algorithms to be processor-heavy because the annotated data needs to be read, and the stastistical computations need to be carried out. This is not an issue when it is done for a large set of text, but if it is done for every sentence identified with a temporal expressions, then the computation-work is much larger than the input. Thereby, more work is being done for the computation than required.
\par Due to the time-constraints, it was decided to use the trimming approach, in specific the algorithm provided by Dorr, Zajic, and Schwartz \cite{dorrzajicschwartz2003} (Figures \ref{alg:trim} and \ref{alg:last}). Where the given input text is turned into a tree based on its grammatical structure, with the words in the text as the leaves, and the inner nodes being the identifiers (given by the POS Treebank\footnote{\url{https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html}}), which is then trimmed. Note that removing a parent node, includes removing its children in the tree.
-cite

\begin{algorithm}[H]
\SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
	\Input{A Grammatical Tree T of the Sentence to summarize}
	\Input{threshold: Threshold value}
	\Output{A Summary of the given sentence}
	get the leftmost-lower subtree with root S\;
	remove time expressions\;
	remove determiners (e.g. 'a', 'the')\;
	\While{the number of leaves in tree $>$ threshold and there are subtrees to remove with this rule}{
		remove all the children except the first, where the rightmost-lowest subtree with root XP and its first child also being an identifier XP (where XP can be NP, VP, or S)\;
	}
	\While{the number of leaves in tree $>$ threshold and there are subtrees to remove with this rule}{
		remove any XP (where XP can be NP,VP,PP) before the first NP found\;
	}
	get Tree T' from lastRule\;
	from T' create a sentence S by reading of the leaves in pre-order\;
	return S\;
\caption{Dorr, B., Zajic, D. and Schwartzm R, (2003). Hedge Trimmer: A Parse-and-Trim Approach to Headline Generation}
\label{alg:trim}
\end{algorithm}
\begin{algorithm}[H]
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{A Grammatical Tree T of the Sentence to summarize}
\Input{threshold: Threshold value}
\Output{A Grammatical Tree of the Summary of the input after the last rule has been applied}
\If{the number of leaves in tree > threshold}{
		make a copy of the tree T'\;
		\While{the number of leaves in tree $>$ threshold and there are subtrees to remove with this rule}{
			remove any trailling PP nodes (and their children)\;
		}
		\If{the number of leaves in tree $>$ threshold}{
			\While{the number of leaves in tree $>$ threshold and there are subtrees to remove with this rule}{
				remove any trailling SBARs (and their children)\;
			}
			\While{the number of leaves in tree $>$ threshold and there are subtrees to remove with this rule}{
				remove any trailling PPs (and their children)\;
			}
		}{
			return T'\;
		}
}
return T\;
\caption{Last Rule}
\label{alg:last}
\end{algorithm}

\section{Data Processing and Representation}
\par In the grammatical tree formed, the inner leaf identifiers are given by the P.O.S Treebank\footnote{\url{http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html}}. Each word or set of words are given a part of speech tag identifying them. An example can be found below (see Figure \ref{tree:origString}).
\par On the grammatical tree shown in the figure, the algorithm proposed by Dorr, Zajic, and Schwartz \cite{dorrzajicschwartz2003} will be applied and the process will be graphically shown in the following figures (see Figures \ref{tree:removeTime}, \ref{tree:removeDeterminers}, \ref{tree:removeDeterminers}, \ref{tree:xpOverXpFirst} and \ref{tree:xpOverXpSecond}). Note that the value of the threshold does not force the summary to be below it, but it is a length of the summary to which the algorithm is working towards. For the sentence: "On Friday the Washington Post came out with the latest from its long-running investigation into Trump's charitable donations.", the following gramamtical tree is produced \ref{tree:origString}.
\begin{figure}[h]
\resizebox{\linewidth}{!}{
\Tree
[.ROOT
	[.S 
		[.PP 
			[.IN \textit{On} ]
               		[.NP 
				[.NNP \textit{Friday} ]
			]
		]
          	[.NP 
			[.DT \textit{the} ]
			[.NNP \textit{Washington} ]
			[.NNP \textit{Post} ]
		]
		[.VP 
			[.VBD \textit{came} ]
			[.PRT 
				[.RP \textit{out} ]
			]
			[.PP 
				[.IN \textit{with} ]
				[.NP 
					[.NP 
						[.DT \textit{the} ] 
						[.JJS \textit{latest} ]
					]
					[.PP 
						[.IN \textit{from} ] 
						[.NP 
							[.PRP \textit{its} ] 
							[.JJ \textit{long-running} ] 
							[.NN \textit{investigation} ]
						]
					]
				]
			]
			[.PP 
				[.IN \textit{into} ]
				[.NP
					[.NP 
						[.NNP\textit{Trump} ]
						[.POS \textit{'s} ]
					]
					[.JJ \textit{charitable} ]
					[.JJ \textit{donations} ]
				]
			]
		]
		[.DOT \textit{.} ]
	]
]
}
\caption{P.O.S/Grammatical Tree of: "On Friday the Washington Post came out with the latest from its long-running investigation into Trump's charitable donations."}
\label{tree:origString}
\end{figure}
\par The inner nodes in the tree are identifiers given by the P.O.S Treebank, and the leaves are the words in the sentence. A parent identifier can be a broader identifier of a collection of sub-identifiers or direct identifiers of a word. Using the Hedge-Trimmer algorithm \cite{dorrzajicschwartz2003}, the tree is procesed as follows.  First, the lowest-leftmost S must be identified, as can be seen from the figure \ref{tree:origString} there is only one subtree with root S. This subtree is  extracted, and the algorithm is continued. The next step is to remove time expressions, of which there is only one, "Friday". However, when removing it, the "NP" parent must be removed, else there is a grammatically incorrect summary (given by the Hedge-Trimmer algorithm  \cite{dorrzajicschwartz2003}). The result is graphically shown in the Figure \ref{tree:removeTime}.
\begin{figure}[h]
\resizebox{\linewidth}{!}{
\Tree
	[.S 
          	[.NP 
			[.DT \textit{the} ]
			[.NNP \textit{Washington} ]
			[.NNP \textit{Post} ]
		]
		[.VP 
			[.VBD \textit{came} ]
			[.PRT 
				[.RP \textit{out} ]
			]
			[.PP 
				[.IN \textit{with} ]
				[.NP 
					[.NP 
						[.DT \textit{the} ] 
						[.JJS \textit{latest} ]
					]
					[.PP 
						[.IN \textit{from} ] 
						[.NP 
							[.PRP \textit{its} ] 
							[.JJ \textit{long-running} ] 
							[.NN \textit{investigation} ]
						]
					]
				]
			]
			[.PP 
				[.IN \textit{into} ]
				[.NP
					[.NP 
						[.NNP\textit{Trump} ]
						[.POS \textit{'s} ]
					]
					[.JJ \textit{charitable} ]
					[.JJ \textit{donations} ]
				]
			]
		]
		[.DOT \textit{.} ]
	]
}
\caption{P.O.S/Grammatical Tree after removing time expressions}
\label{tree:removeTime}
\end{figure}
\par On the resulting tree, the algorithm dictates the removal of determiners. A determiner is identified by the "DT" parent tag. However, not all the parent identifiers with this tag are removed, only ones that have a child labelled "the" or "a". The resulting tree after applying this rule is given by Figure \ref{tree:removeDeterminers}.
\begin{figure}[h]
\resizebox{\linewidth}{!}{
\Tree
	[.S 
          	[.NP 
			[.NNP \textit{Washington} ]
			[.NNP \textit{Post} ]
		]
		[.VP 
			[.VBD \textit{came} ]
			[.PRT 
				[.RP \textit{out} ]
			]
			[.PP 
				[.IN \textit{with} ]
				[.NP 
					[.NP 
						[.JJS \textit{latest} ]
					]
					[.PP 
						[.IN \textit{from} ] 
						[.NP 
							[.PRP \textit{its} ] 
							[.JJ \textit{long-running} ] 
							[.NN \textit{investigation} ]
						]
					]
				]
			]
			[.PP 
				[.IN \textit{into} ]
				[.NP
					[.NP 
						[.NNP\textit{Trump} ]
						[.POS \textit{'s} ]
					]
					[.JJ \textit{charitable} ]
					[.JJ \textit{donations} ]
				]
			]
		]
		[.DOT \textit{.} ]
	]
}
\caption{P.O.S/Grammatical Tree after removing determiners}
\label{tree:removeDeterminers}
\end{figure}
\par The threshold value is used in the following rules. For this example, the threshold value is 10. From the two rules left, only one will be applied as the number of leaves in the resulting tree will fall below the threshold value. Thereby, the summary is below the threshold, so the last rule rule will not be applied. This avoids over-trimming the summary.
\par The threshold's value importance is of stopping the algorithm from over-trimming or under-trimming. This is done in the sense of there being an optimal summary where the core meaning is kept but if it is further reduced the meaning is lost. When the tree is over-trimmed there is a possibility that the resulting summary does not have the core meaning of the original sentence, or has no meaning at all. The advantage of under-trimming is that the meaning of the sentence is very likely kept, due to the summary sentence being of greater length than the optimal summary. Thereby, having more words and thus retaining the original meaning, as it is closer to the original sentence. However, the aim is to produce a summary, i.e. a short sentence with the same meaning as the original. Under-trimming can ensure the meaning is kept in the summary, but not that the resulting summary is optimal in size (thereby incrementing the time required by the user to view the entire timeline, thus increasing the time the to understand what occurred, which is what the user aimed to avoid with the tool).
\par When applying the XP-Over-XP rule, a parent identifier labelled XP (where XP can be NP,VP or S), has a first child identifier of type XP also, then all other children of the parent are removed. This rule is done iteratively until the resulting tree is below the threshold, or the rule cannot be applied further due to there not being anymore parent XP-first child XP pairs. The first iteration of the rule is shown in Figure \ref{tree:xpOverXpFirst}, and the second and final iteration in Figure \ref{tree:xpOverXpSecond}.
\begin{figure}[h]
\resizebox{\linewidth}{!}{
\Tree
	[.S 
          	[.NP 
			[.NNP \textit{Washington} ]
			[.NNP \textit{Post} ]
		]
		[.VP 
			[.VBD \textit{came} ]
			[.PRT 
				[.RP \textit{out} ]
			]
			[.PP 
				[.IN \textit{with} ]
				[.NP 
					[.NP 
						[.JJS \textit{latest} ]
					]
					[.PP 
						[.IN \textit{from} ] 
						[.NP 
							[.PRP \textit{its} ] 
							[.JJ \textit{long-running} ] 
							[.NN \textit{investigation} ]
						]
					]
				]
			]
			[.PP 
				[.IN \textit{into} ]
				[.NP
					[.NP 
						[.NNP\textit{Trump} ]
						[.POS \textit{'s} ]
					]
				]
			]
		]
		[.DOT \textit{.} ]
	]
}
\caption{P.O.S/Grammatical Tree after XP-Over-XP first iteration}
\label{tree:xpOverXpFirst}
\end{figure}
\begin{figure}[h]
\resizebox{\linewidth}{!}{
\Tree
	[.S 
          	[.NP 
			[.NNP \textit{Washington} ]
			[.NNP \textit{Post} ]
		]
		[.VP 
			[.VBD \textit{came} ]
			[.PRT 
				[.RP \textit{out} ]
			]
			[.PP 
				[.IN \textit{with} ]
				[.NP 
					[.NP 
						[.JJS \textit{latest} ]
					]
				]
			]
			[.PP 
				[.IN \textit{into} ]
				[.NP
					[.NP 
						[.NNP\textit{Trump} ]
						[.POS \textit{'s} ]
					]
				]
			]
		]
		[.DOT \textit{.} ]
	]
}
\caption{P.O.S/Grammatical Tree after XP-Over-XP second iteration}
\label{tree:xpOverXpSecond}
\end{figure}
\par If the number of leaves did not fall under the threshold value, then the next rule to be applied is XP-Before-NP. In this rule, any XP before the NP of the sentence (the grammatical subject) is removed. This would be carried out until the number of leaves falls below the threshold, or the rule cannot be applied further. Finally, the last rule is applied.  This rule consists of iteratively removing trailing PP and SBAR subtrees until the threshold value is reached, or the rule cannot be applied further. 
\par The result of applying this algorithm (Hedge Trimmer  \cite{dorrzajicschwartz2003}) on the input text: "On Friday the Washington Post came out with the latest from its long-running investigation into Trump's charitable donations.", produces a summary: "Washington Post came out with latest into Trump's". The original meaning of the sentence, which is that the Washington Post released a new article on Trump, is kept. However, the resulting summary is not grammatically correct as "Trump's" should be "Trump". Even though the summary is not grammatically correct the summary is significantly shorter than the input text, and does convey the meaning of the original sentence. This is a headline of the input, indicating what the input text was about, and thereby giving a general description of what occurred.
-tags are POS (cite) //d
-example of input text to tree //d
-example of producing summary //d 
-algorithm //d 
-what are the options for the summary (Neural Networks vs Decision-Based) //d
-give an algorithm for determining the summary, with an example //d

-explain the date problem, with example (determine that it uses an ISO standard)
\section{Normalizing Dates}
\par An issue when processing documents, and identifying events, is that sentences will not always include the exact date of when an event occurred. These temporal expressions are ambigious. For example, "Yesterday" or "Last week" have different meanings depending on the context in which they are in. To produce a timeline, exact dates are required to sort the events by their date. This is valuable to the user, as events that occur within the same time era will be grouped and separated from events of other time eras. To determine the exact date of an ambigious temporal expression, a reference point can be used. For example, if the text was written on the 11th of March 2017, then "Yesterday" in this context refers to the 10th of March 2017, but if the document was written on the 1th of February 1689, then it refers to the 31st of January 1689. Thereby, it is important to be able to infer reliably and accurately the date to which ambigious time expressions refer. 

\par The reference point (or base date) acts as a context of when an event occurred. Reference points, and similar techniques have been used in other timeline works \cite{mccloskymanning2012}. The reference point should be the context in which the document was written in, or was aimed to be written in. This can be the publishing date of an article, or the creation date of a document, or any date which allows the exact date of an ambigious temporal expression to be determined. For exact dates that are described in text, such as "On the 12th of December 1996...", the reference point has no value, as it can be determined without it that this text refers to an event that occurred on 12-12-1996. Therefore, the reference only helps the task of producing exact dates.

\par An issue that may arise is that temporal expressions point to a range of dates, i.e. "In the 1980s...". While it is not possible to determine the exact date, or dates, in which an event occurred it can be determined reasonably \cite{mccloskymanning2012} that it was somewhere between the start of 1980s, i.e. 01-01-1980, and the end of the 1980s, i.e. 31-12-1989. In an attempt to produce an exact date, a range of start and end date can be used for this event. This allows for it to be compared to other events, to then sort, and inform the user the event occurred somewhere within that time period. 


