\chapter{Background}
The background should set the project into context by motivating the subject matter and relating it to existing published work. The background will include a critical evaluation of the existing literature in the area in which your project work is based and should lead the reader to understand how your work is motivated by and related to existing work.

//explain what an event (from what is it built off)
The resulting system aims to produce a timeline of events based on the input text. An event is given by its date(s), subjects and a short summary of the sentence that produced it. An event is produced when a given sentence contains a date. An event can have more than 1 date if it is considered to happen in a range of dates. For example, an event that happened in the 1980s would have two dates, one for the start date: 1980-01-01, and one for the end date: 1989-12-31. While an event that happened just on one day would have just one date. The subjects of an event are given by the "person, place, thing, or idea that is doing or being something"\cite{grammar}.

\section{Natural Language Processing}
-explain what NLP is
\par The projects primary area of research is Natural Language Processing (NLP). NLP is the area of computer science where the aim is to translate human readable and spoken language to a computer (cite). This requires the human input to be subject to constraints. Thereby in this project, expections on the input text are assumed. For example, it is to be expected that documents are written in correct English. As every language has their own grammar, and thereby, their own rules, to expand the system to different languages would require different rule sets to be applied depending on the language to process the text. This includes the algorithms used in the summary of sentences used in events (discussed in a latter section).
\par A relevant issue is the input documents having text that is disorganised in a grammatical sense. Many NLP software tools (incluiding StanfordCoreNLP used in this project) perform extremely poorly with such input text. For example, in the paper Named Entity Recognition in Tweets: An Experimental Study \cite{ritter2011}, where they looked at the performance of popular NLP tools on "Tweets", which due to them being limited to a character count will use abbrevations that do not make sense grammatically. This is due to the fact that the NLP tools and algorithms cannot apply their rules and models to the text to identify the different components. Thereby, in this project the assumption is that the input will be in correct English, as the systems primary user is a law professional. NLP will be presented in more detail in the following section.
-what parts of NLP are involved
\par NLP is a broad area of study. However, in this project the focus is on Automatic Summarization, Named-Entity Recognition (NER), and Sentence Breaking.
\par In Automatic Summarization the aim is to produce a shorter version (the summary) of a given input text, that still holds the same meaning of the original input. The summary can be built directly from the words in the input, or it can built using a dictionary. As in this project, an event is built from one sentence that contains a date, the specific area of Automatic Summarization which was focused on was Headline Generation. This is where a summary is built based on a given input text, such that the summary falls below a certain threshold value. For Headline Generation there are two main implementations: statistic based and decision (or trimming) based \cite{daumemarcu2002}.
\par In the statistic based model, where Noisy-Channel models are the most prominent, as shown by the multitude of publications \cite{daumemarcu2002,rushchopraweston2015,chopraaulirush2016}. In noisy-channel models, the belief is that the summary of the given input lies within the text but it is surrounded by unwanted noise (text). These systems require a large collection of annotated data (pairs of input and their summary), which is used in the calculation of the statistics of whether or not a produced summary correctly represents the input. Examples of these algorithms can be found in the works of \cite{daumemarcu2002,knightmarcu2000}.
\par The decision based models, are older than the statistic based models and use the grammar of the input text to trim (remove) parts of the inputs until no more rules can be applied or the summary produced is below a given threshold \cite{dorrzajicschwartz2003}. This is done by tokenizing, breaking an input text into words, phrases, symbols and tagging them each by an identifier(cite). The tokenized text, which is usually represented as a tree where the leaves are the words in the input text and the inner children the identifiers, is passed through an algorithm which applies rules which removes branches of the tree until no more rules can be applied or the summary text is below a given threshold. The trimmed tree is then used to produce the summary.
//compare statistics to decision
\par The trimming based models do not tend to produce as good of summaries as the statistic based model, due to them producing, usually, only one summary while the statistic based models produce a selection to choose from. However, as can be seen from the works of Knight and Marcu \cite{knightmarcu2000}, the trimming based models can produce better summaries than the statistic models in some occassions. The main advantage of the trimming model is their speed, and not requiring a large corpus of data (like the statistic models) by relying on the grammar to build the summary. In newer works of text summarization, neural network models are being used. These fall under the statistical based models. They produce extremely accurate results, but as most statistical models they require a large corpus of data. They requirement of the extensive sample of annotated data also leads to these algorithms being processor-heavy, as the data needs to be read, and calculations need to be performed to produce the probability values used in identifying suitable summaries.
\par Due to the time-constraints of the project, it was decided to use the trimming approach, in specific the algorithm provided by Dorr, Zajic, and Schwartz \cite{dorrzajicschwartz2003} (figure of algorthim). Where the given input text is turned into a tree based on its grammatical structure, with the words in the text as the leaves, and the inner nodes being the identifiers, which is then trimmed. In addition, in statistical models the input text size is multiple sentences, i.e. a paragraph or more of text, where loading the models from the annotated data has less of an impact in performance as this being done for every sentence in that input text. 
-cite
\begin{algorithm}
\SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
	\underline{function GenerateSummary} $(a)$\;
	\Input{A Grammatical Tree of the Sentence to Summarize}
	\Output{A Summary of the given Input}
	get the lowest-leftmost S inner node\;
	remove determiners\;
	do xpOverXp\;
	do xpBeforeNp\;
	\eIf{above threshold}{
		do remove PPs\;
		\eIf{above threshold}{
			undo previous\;
			remove SBARs\;
			remove PPs\;
		}
	}
	turn the tree into a String S\;
	return S\;
\caption{Dorr, B., Zajic, D. and Schwartzm R, (2003). Hedge Trimmer: A Parse-and-Trim Approach to Headline Generation}
\label{alg:trim}
\end{algorithm}
\section{Data Processing and Representation}
\par In the grammatical tree formed, the inner leave identifiers are given by the P.O.S Treebank (cite). Each word or set of words are given a part of speech tag identifying them. An example can be found below. (example figure).
\par On the grammatical tree shown in the previous figure, we will apply the algorithm proposed by Dorr, Zajic, and Schwartz \cite{dorrzajicschwartz2003}. This can be shown graphically in the following figure. It is to be noted that the value of the threshold does not force the summary to be below it, but it is a length of the summary to which the algorithm is working to.
-tags are POS (cite)
-example of input text to tree
-example of producing summary
-algorithm
-what are the options for the summary (Neural Networks vs Decision-Based)
-give an algorithm for determining the summary, with an example
-explain the date problem, with example (determine that it uses an ISO standard)



//defintion of an event

