\chapter{Evaluation}
-present how did analysis, why?, other options (relate to work)
-results of analysis
-conclusion
\par As presented in the Design chapter, the objectives of the system are: visibility, efficiency, and effectiveness. To evaluate each of these, the chapter is split into three sections. Each addressing an objective.

%\section{Software Testing}
\section{Visibility}
-explain expert heuristic
-knowledge of expert
-that this is the least important of the three as the ui can be built by third party and only processing is used.

\par To evaluate the visibility of the system, an expert heuristic using the Nielsen Usability Heuristic \cite{nielsen1995} set was used. This involves an expert evaluating the UI of the system with a set of principles, which have been tested, and then discussing the system for each principle. The evaluation can be formal or informal. The former was choosen, as it was carried out at the end of the development. Informal evaluations are done when analysis needs to be done during the iterations of design, since it provides feedback quicker and allows designers to discuss the problems when producing the next design. Formal evaluation is done, usually, at the end of the development cycle when the system is being evaluated. In this case, the focus is on evaluating visibility. Heuristics do not only focus on visibility, since some of the principles focus on error prevention. However, these are also important for a system that is to be simple to use.

\par Other heuristic sets exist, however the Nielsen set is the most known set\footnote{\url{https://www.usability.gov/how-to-and-tools/methods/heuristic-evaluation.html}}. Another prominent design evaluation is cognitive walkthroughs. They consist of giving experts, scenarios to go through, and during their progression of the scenarios, they  answer three questions related to design issues (cite).

\par Due to time constraint, only one in-depth expert heuristic could be carried out. Normally, there would be multiple experts, each carrying out their own evaluation, and then they would discuss between themselves to produce a single document detailing the usability problems. In this case, the expert has experience in applying the principles from previous projects. Note that they do have technical knowledge in Computer Science, hence they are not considering the system from a novice users perspective, but rather in identifying where the system satisfies the principles. Experts may be biased in how they believe a design should be, however this is expeced as designs are a subjective matter.

\par The data collected from an expert review is qualitative data. Qualitative data is statements, observations and subjective judgements. In this case, they are issues in the system with possible solutions, which aim to satisfy the principles they violate. It is different to the quantitative (numerical) data collected in the Effectiveness section.

%include expert heuristic table in appendix (ie point to it) or include it here
\par The main principles evaulated are visibility, match with the real world, consistency and error prevention. With a main focus on visibility..

\par For the visibility principle, where the focus is on the user knowing the systems state, the design succeeds according to the expert. The user is aware of the actions available to them as the system progresses through its states of processing files. This begins from the system only allowing the user to load documents on startup, to the loading dialog when the documents are being processed, and to the subsequent viewing of the produced timeline. To ensure error prevention, where data validation is done on user input, red-borders are shown in the input fields when the data is invalid and the options to save the data are disabled. This allows the user to determine that a mistake was made with the input, and specifies where the mistake occurred. To ensure the user knows what the expected input format is, the input fields are aided with hints. For example, the format of dates is shown when the user must enter the base date for a document, or a character limit is shown when the user is editing the summary of an event. Clearly marked exit, "Cancel" buttons, are provided to allow the user to not commit their changes. Confirmation dialogs are also used to prompt the user to confirm an action that may be costly, e.g. deleting the subjects of an event, or deleting the event itself. All the previously mentioned design choices, accompanied with the non-technical language used in the system, ensure that the system should be accessible to any user.

%expert suggestions
\par To improve visibility, the expert suggested improvements. For example, hinting the user on how to delete subjects, or providing an indicator for the zooming in/put function in the "Range View". To aid the user recognition of the function of buttons, it was suggested to use icons that visually describe the function. The expert suggested filling up a progress bar, instead of the circular progress bar in the system, when documents are processed. However, this is not a clear indication of the progress as some documents require more time to process than others, but in the progress bar they would have equal impact.

\par From this evaluation it can be determined that the system fulfilled its aim of producing a system that is visible to the user. The user is informed of the states the system is in as it processes documents. Clearly marked exists, and error prevention with indicators of what the error is, are provided. Note that visibility is not as important as the other objectives, as the system could be used as a library (directly, or indirectly through JSON outputs) where another developer produces their own UI. However, in the case that the system is used by a user (not a developer), the UI produced is accessible for them, and also provides useful information.

\section{Efficiency}
\par Efficiency relates to the time it takes for the system to produce an answer. A system is efficient if it produces a result in an appropriate amount of time based on its input.

\par A way to evaluate this is to run the system on different inputs and record the time taken to produce a timeline. However, the time recorded is specific not only to the machine that the system is being run on, but also how the CPU scheduled the work at that point in time (i.e. how much other work was being carried out on the machine). If the system is ran with other tasks running in the background and the CPU gives the application less priority, i.e. other tasks run before this one, then the system will require more time to produce a result. Note that depending on the hardware used when running the system, results may be produced quicker or slower (cite). In addition, not all input sizes can be tested, as the document set size can be infinite. The solution to this is to consider the time complexity of the main algorithms used in the system. 

\par The time complexity of an algorithm is, the number of operations, and time required to produce an output for an input of a given size (cite). This allows to determine how well the system scales with larger inputs, by only considering how many operations are performed as a function of the input.

\par Time complexity focuses on the worst case scenario, and does not consider low-order operations. For example, if for an input $n$ an algorithm performs $3n+2$ operations, the low-order term $2$ and the co-efficient $3$ are omitted. This would result in a time complexity $O(n)$ (where $O$ is called big-Oh). In general the highest order-term is taken, and all the rest are omitted. For example, $4n^2+10n+2$ would have time complexity $O(n^2)$.  Low-order terms are omitted since with a very large input, or as ${n\to\infty}$ ($n$ tends to infinity), the time required for the low-oorder operations becomes irrelevant. The co-efficient of the high-order term is omitted, as a machine can be 4 times faster, or 4 times slower, so it does not affect, in general, the number of operations performed.

\par To measure the efficiency of the system, the time complexity of the front-end (graphical part) and back-end (logical part) of the system have been computed separately, as the system can be used standalone, or as a library in other systems that produce their own UI. The complexity of each algorithm is presented in the table below (see Figure \ref{fig:timecomplexityTable}).

\begin{figure}[h]
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
Algorithm & Information & Time Complexity\\
\hline
\hline
Document Processing &  \begin{tabular}[x]{@{}c@{}}$n$ - number of documents\\$s$ - number of sentences\\$w$ - number of words\end{tabular} & $O(nsw^2)$ \\
\hline
Range Production & $n$ - number of Results & $O(n^2)$\\
\hline
Range Timeline View & $n$ - number of Results & $O(n)$\\
\hline
Traditional Timeline View & $n$ - number of Results & $O(n)$\\
 \hline
\end{tabular}
\end{center}
\caption{Time complexity of main Algorithms in System}
\label{fig:timecomplexityTable}
\end{figure}

\subsection{Back-End}
\subsubsection{Processing Documents}
\par The two main algorithms are processing of documents, and the production of Ranges. The complexity of each is presented and discussed. It is assumed that the time complexity of the StanfordCoreNLP to annotate a document is $O(w)$ (for all $w$ words in the document), and that the documents are annotated before being processed.

\par The algorithm used was previously presented (see Algorithm \ref{alg:processFiles}). For a list of $n$ documents, each is processed. The algorithm can process at most $x$ documents in parallel at any time. Depending on the users settings value $x$ can be 1 or $\infty$ (infinity). If $\infty$, then it can be determined that the complexity of the algorithm is given by the complexity of processing the largest document, since all documents are being processed in parallel. As all documents are processed in parallel. 

\par When a document is processed, each sentence is checked for a temporal expression, before a summary or any other processing is done. In the worst case, all sentences $s$ in a document have to be fully processed. In such a case, the dates, the subjects, and the summary need to be produced. These are all done after each other, so it can be determined that the computation complexity of processing one sentence is given by $max(getDate, getSubjects, getSummary)$. Where $max$ will return the greates time complexity of the three operations. 

\par To get the date of a sentence, with $w$ words, each temporal expression needs to be processed. The processing of a temporal expression is linear (see the getDate implementation in Figure \ref{fig:refCode}), as it does not depend on the input directly. It performs at most 3 loops to produce an exact date. Thus processing an NER date has a complexity of $O(1)$. In the worst case, every word in the sentence is a temporal expression (which does not happen normally in a sentence of well-written documents, but can still occur). Thereby, the time complexity for getDate is $O(w)$.

\par To select the subjects of a sentence, the NER(footnote) annotator is used to determine which words are of interest. In the worst case, all words can be of interest. When a word is identified it is placed in a list, for the subjects of that event. Placing a word in a list has time complexity $O(1)$, hence it being done for $w$ words, in a sentence, leads to a complexity of $O(w)$.

\par To create the summary of a sentence, with $w$ words, the hedge-trimmer algorithm is used (see the Algorithm presented in the Design Chapter). In the paper, the time complexity of the algorithm is not presented as the algorithm is not explicitly provided, but rather explained. Thereby, the algorithm that was implemented is analyzed. In the grammatical tree produced, each word is a leaf. The structure of the tree varies, however for this evaluation it is assumed that in the worst case a full-binary tree is produced. This may not be the case, as the trees produce vary in format, however it is required to proceed with the evaluation. If there are $w$ leaves in the tree (i.e. each word in the sentence is a leaf), then there are $2w-1$ nodes in the tree. The rules of the algorithm are applied one after the other. Thereby, the time complexity of the algorithm is given by the rule with the highest time complexity. The first two rules traverse the tree once, and have a complexity of $O(2w-1) = O(w)$. In the last step, where the tree is iteratively shortened until it is below the threshold. In the worst case, the threshold can be 0 (not explicitly allowed in the system as the minimum value is 10). Note that the threshold value is met when the number of words in the summary falls below it, and that the size of the summary is given by the number of leaves in the current grammatical tree. At each step, at least one node needs to be removed for the algorithm to continue until the threshold is met. Note that if an inner node is removed, its children are also removed. Thus, the leafs are reduced, thereby getting closer to the threshold value. To identify the node to remove, the tree must be traversed, and as presented earlier, this has complexity $O(w)$. In the worst case, only one leaf is removed each time. Thus the tree is traversed $w$ times. Therefore the time complexity of the last rule is given by $O(w^2)$.

\par Therefore, the processing of a sentence with $w$ words is given by $O(w)+O(w)+O(w^2)$, where each time complexity corresponds to a task (infering dates, getting subjects, and producing a summary). Thereby, the time complexity of processing a sentence is $O(w^2)$, as the low order terms are removed. 
\par Assuming that the set of $n$ documents is processed one after the other (i.e. at most 1 document can be produced in parallel, in the worst case), and that all the documents have $s$ sentences (or less). Where each sentence has $w$ words (or less). The running time of processing one document is $O(sw^2)$. Thus the running time of processing $n$ documents is $O(nsw^2)$. Where it is not known which of $n$, $s$, or $w$ is the high-order term. The time complexity of annotating a document is omitted as it can be considered that $sw^2 > w$.

\par If the input consists of many documents, with few short sentences (i.e. $w$ and $s$ are smaller than $n$), then the running time is given by $O(n)$. Which for processing $n$ documents, is efficient since the system scales linearly with the input. However, this is not always the case, as it may be that there are many short sentences (i.e. $s$ is greater than $n$ and $w$, thus the time complexity is $O(s)$), or there are few long sentences (i.e. $w$ is greater than $n$ and $s$, thus the time complexity is $O(w)$).

\subsubsection{Range Production}
\par Ranges are produced with the algorithm presented in the Design Chapter. Based on an input of $n$ Results, they have to be sorted, added to existing Range trees or else creating new Range trees, and then, finally, the trees must be sorted.

\par Sorting Results in Java has a complexity of $O(n\log n)$. This is due to Java using merge-sort when comparing the results (footnote). Merge-sort consits of recursively breaking down the problem space in half (which produces the $\log n$ part), and then building the sequence back up (which produces the $n$). However, it should be noted that with sequences that are almost sorted Java 8+, due to the use of TimSort\footnote{\url{https://bugs.openjdk.java.net/browse/JDK-6804124}}, the time complexity is closer to$O(n)$. 

\par Each Result needs to be added to a tree. Before the tree is traversed to find a location, a check is performed to determine whether it needs to be added to the tree. In the worst case, the Result cannot be added to any tree, thus all the trees are checked and a new Range tree needs to be amde to hold the Result. This can be the case when the dates of events are disjoint.  In such cases, the amount of Trees that need to be checked increase by 1 for each Result added. Assuming that the time complexity to check whether a Result can be added to a tree is $O(1)$ (since the check is in constant time), then the time complexity is given by the sum of 0 to $n$. As in the first step, no Ranges exist so nothing needs to be checked. Then one tree needs to be checked, then two trees, and so on. Where at the $n$th Result, $n-1$ trees need to be checked, each with a complexity $O(1)$. Thereby, the total complexity is given by the sum of 0 to $n$, which is given by ${n(n+1)}/2$. This produces a time complexity $O({n^2+n}/2)$, which is $O(n^2)$.

\par After all the trees have been produced, the Ranges need to be sorted. In the worst case there is one full expanded tree. Which is a tree where for each Result, it had to be expanded, because the dates overlapped (but where never fully contained within each other). This leads to a full binary tree as when a Range is expanded it has two children set to it: one being a new Range holding the Result being added, and the other, the previous subtree that the Result's dates partially overlapped with. Which using the Java sorting algorithm leads to $O(n\log n)$, as the low-order terms are dismissed.

\par Thereby, the time complexity of the Range Production algorithm is given by the operation with the greatest complexity, which is adding Results to the tree. Thus, the time complexity of the algorithm is given by $O(n^2)$.

\subsection{Front-End}
\par In this section the focus is on the creation of the range and traditional timeline views.
\subsubsection{Range View}
\par As can be seen by the algorithm to produce the Range view (see the Figure \ref{alg:rangeLayout} in the Implementation Chapter), each Range is considered separately when it is built. In the worst case, there is one Range with a fully expanded tree (i.e. a full binary Range tree). If the tree holds $n$ results, which are held at the leaves, then it has a total of $2n-1$ nodes. For all nodes it must produce a layout, which includes the production of its children recursively. Therefore, $2n-1$ layouts are produced, so the time complexity is given by $O(n)$. 

\par There can be performance issues, as this layout embeds layouts within each other and many views are held in memory at a time. However, the focus in this section is to consider the time for the system to produce the UI, not how heavy it is for the system's memory.

\subsubsection{Timeline UI}
\par The creation of the timeline view is trivial. For a list of $n$ results, the layout for each row of the ListView is produced. Hence, $n$ rows are produced. This leads to a time complexity of $O(n)$.

\par Note that the ListView used in the system does not hold all rows in memory at once. The layout of a row is only produced if it needs to be shown, i.e. that part of the ListView is visible to the user (cite). Hence, the time complexity to produce the timeline view is less than $O(n)$, as the system does not show all the Results. Only when the size of the ListView is smaller than the amount of allocated screen space it is given, will it have to produce $n$ rows as all rows can be shown, and thus will have a time complexity of $O(n)$.

\section{Effectiveness}
-how did you test?
-participants data?
-safeguard participants
-data set
-why test this way
\par Effectiveness of a system is how correct it is at producing results, in this case timelines. To determine whether a timeline is correct, experimental testing was done. This allows to determine whether the produced timeline is similar to ones produced by people.

\subsection{Testing Preparations}
\par In order to test the effectiveness of the timelines, test participants were asked to build manual timelines given by a document. These were then compared to the produced timeline by the system for the same document.

\par Note that there is no one perfect timeline, but instead there can be many good timelines that express the documents. This is demonstrated by the different timelines produced by the test participants for the same document.  The aim is to compare the produced timelines to the manual timelines, and see what are the differences and similarities.

\par Participants were kept anonymous during the entire process, and will be kept anonymous during the presentation of data. The participants came from different academic backgrounds, were of different ages, and had different levels of exposure to the document data sets. A clear limitation to the experiment was the availability of participants. As such, limited data was gathered.
 
\par The data sets consisted of publicly available newspaper articles of different domains: politics, criminal cases, and short bibliographys. To ensure that a substantial amount of data could be gathered, articles that were rich in temporal expressions were picked. This allows for more timeline data points to be compared between a manual timeline and an automated produced timeline.

\par Using publicly available data sets (e.g. newspapers), and keeping the participants anonymous complies with the guidelines of the BCS, and does not require an ethical approval for the experiment.

\subsection{Testing}
\par Participants were provided a document, and were asked to select events line-by-line, like the system. This is to allow comparison between the two timelines. 

\par There was no time limit for the participants, as the aim was for them to produce correct timelines instead of producing timelines quickly, since the system has a clear advantage in that regard.

\par The manual events produced by the participants, included the date of the event and a short summary of what occurred. In situations were the dates cannot be determined, participants would link the events to others by saying it occurred before/after another event. Currently, the system cannot establish these links.

\par The manual and automated produced timelines are found in the Appendix.

\subsection{Analysis}
\par The comparison of the manual timelines consisted in: the number of events collected, the percentage of matched and unmatched sentences, the percentage of mismatched exact dates, and the similarity in summaries. 

\par The number of events allows to determine what is the expected amount of events for a given document. This can be combined with the other data to determine if the timeline produced is to broad in its event generation, or too specific.

\par The matching and unmatching of sentences, consists of identifying which sentences the users used in their generation of events and which the system used. This allows to evaluate how effective the system is in identifying an event, independently of whether the data of the generated event is correct.

\par Mismatched dates consists of determining how accurate the system is in producing exact dates for events. It is expected that the system will perform poorly in situations were the date of an event cannot be identified, but only that it occurred before/after another event. For test participants this is not a problem, as they understand context and can apply their own thinking to determine whether an event occurred before or after another event, even when there is no temporal data to support this. To determine the quantity of mismatched dates, events from the manual and produced timeline are only considered if they originate from the same sentence in the document. Then the dates of the two events are compared.

\par The similarities in summaries determines how effective the system is in conveying the meaning of the original sentence that produced it. The events are grouped by the sentence that produced them, and then it is checked whether the summary and key words of the event, from the produced timeline, matches the meaning of the summary of the event from the manual timeline. This is generally done by checking that they have the same keywords.

\par The results of the following are presented in the table below (see Figure \ref{fig:tableAnalysis}). The name of the articles used are provided, along with their source and their publishing date.

\begin{figure}[h]
\begin{subfigure}{.5\textwidth}
\begin{tabular}{ |p{5cm}|c|c|c| } 
 \hline
Comparing System to Manual Timelines From... & Participant 1 & Participant 2 & Participant 3\\
\hline
\hline
Difference In collected events (+/-) (19 events collected by system) &  +5 & -1 & -3 \\
\hline
Percentage (\%) of events originating from the same sentence & 66.7 & 83.3 & 87.5\\
\hline
Percentage (\%) of events not identified & 33.3 & 16.7 & 12.5\\
\hline
Percentage (\%) of mismatched dates & 0.0 & 6.7 & 0.0\\
\hline
Percentage (\%) of similar summaries (matching keywords) & 75.0 & 73.3 & 78.6\\
\hline
Unkown Dates of Manual Timelines From... & 4 & 2 & 2\\
\hline
\end{tabular}
\caption{BBC: US elections 2016: Donald Trump Life Story (Published 20th of January 2017)}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\begin{tabular}{ |p{5cm}|c|c|c| } 
 \hline
Comparing System to Manual Timelines From... & Participant 1 & Participant 2 & Participant 3\\
\hline
\hline
Difference In collected events (+/-) (4 events collected by system) &  +5 & 0 & +3 \\
\hline
Percentage (\%) of events originating from the same sentence & 44.4 & 100.0 & 57.1\\
\hline
Percentage (\%) of events not identified & 55.6 & 0.0 & 42.9\\
\hline
Percentage (\%) of mismatched dates & 0.0 & 0.0 & 0.0\\
\hline
Percentage (\%) of similar summaries (matching keywords) & 50.0 & 50.0 & 50.0\\
\hline
Unkown Dates of Manual Timelines From... & 0 & 0 & -1\\
\hline
\end{tabular}
\caption{Guardian: British Toddle Abducted, Police believe (Published 5th of May 2007)}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\begin{tabular}{ |p{5cm}|c|c|c| } 
 \hline
Comparing System to Manual Timelines From... & Participant 1 & Participant 2 & Participant 3\\
\hline
\hline
Difference In collected events (+/-) (6 events collected by system) &  +2 & +1 & +1 \\
\hline
Percentage (\%) of events originating from the same sentence  & 75.0 & 88.5 & 71.4\\
\hline
Percentage (\%) of events not identified & 25.0 & 12.5 & 28.6\\
\hline
Percentage (\%) of mismatched dates (on matched events) & 33.3 & 16.7 & 0.0\\
\hline
Percentage (\%) of similar summaries (matching keywords) & 66.7 & 66.7 & 80.0\\
\hline
Unkown Dates of Manual Timelines From... & 1 & 0 & 1\\
\hline
\end{tabular}
\caption{Guardian: Madeleine McCann detectives arrive in Portugal to question 11 suspects (Published 9th of December 2014)}
\end{subfigure}

\caption{Manual vs Automated Timelines Comparison}
\label{fig:tableAnalysis}
\end{figure}

%explain that same sentence means from system to timeline 
%what can be concluded (ie on average summary ...), and final conclusion of system
\par Note that the percentage of events originating from the same sentence focuses on how many events in the manual timeline were also identified in the automated timeline. This allows to determine how many of the events in the manual timeline are not identified by the automated system.

\par No stastical analysis could be performed to determine with certainty how accurate the system is, as there is not enough data. However, it can be concluded, that from the data gathered, that the system identifies more than 75\% of the events in the documents, if the manual timelines identify all events.  Events with completely unknown dates (i.e. there are no explicit temporal expressions) are not identified. The summary generation is not optimal, as just above 65\% of the automated timelines capture the meaning of the manual timelines. Thus, showing the weakness of the decision-based summary algorithms, and reasons as to why the domain-dependent stastic models are more prominent. An effective point of the system was the production of exact dates from temporal expressions. For the automated inference of dates, the majority of inferred dates match the dates determined by the test participants.

\par Therefore, the system does not produce perfect summaries that convey the same meaning as the original sentence. However, the system is effective in the production of exact dates from temporal expressions, and does perform fairly well in identifying events.


































