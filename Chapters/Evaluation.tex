\chapter{Evaluation}
-present how did analysis, why?, other options (relate to work)
-results of analysis
-conclusion
\par As was presented in the Design chapter, the objectives of the system are: visibility, efficiency, and effectiveness. To evaluate each of these the chapter is split into three sections. Each section adresses each keypoint.
%\section{Software Testing}
\section{Visibility}
-explain expert heuristic
-knowledge of expert
-that this is the least important of the three as the ui can be built by third party and only processing is used.
\par To evaluate the visibility of a system a expert heuristic aided by the Nielsen usability heuristics \cite{nielsen1995}. It involves having an expert evaluate the user interface of a system using a set of principles, which have been tested, and them discussing the system for each principle in a document. The evaluation can be formal or informal. The former was choosen to evaulate the system, because it was done at the end of the development. Informal evaluation is done when analysis needs to be done during the iterations of design, as it provides feedback sooner and allows for designers to discuss the problems. Formal evaluation is done, usually, at the end of the development cycle when the system is evaluated on whether it met its objective. In this case, visibility. The heuristics do not only focus on visibility, as some principles focus on error prevention. However, these are also important for a system that is to be used by any kind of user.
\par Other heuristics exist, however the Nielsen set is the most known set\footnote{\url{https://www.usability.gov/how-to-and-tools/methods/heuristic-evaluation.html}}. Another prominent design evaluation are cognitive walkthroughs. They consist of giving experts scenarios to go through, and during their progression they  answer three questions related to design issues (cite).
\par Due to time constraint, one in-depth expert heuristic could only be carried, as opposed to having multiple experts that discuss with each other to produce one document that discusses the principles in context of the system. The expert has had experience in using the principles, and has knowledge in the field of Computer Science. It should be noted that because of the background knowledge of the expert, they are not considering the system from a completely novice user perspective. However, an expert must only consider the principles and not how the system functions. The experts possible bias is a known problem with expert heuristic, however design is a subjective area. It is hoped that the main issues of the system are spotted by the expert user, as the principles are intended to guide them.
\par The data collected from the expert review is qualitative data. Qualitative data are statements, observations and subjective judgements. In this case, they are possible issues in the system with possible solutions, or ways the system fulfills certain principles. It is different to the quantitative data collected in the Effective section.
%include expert heuristic table in appendix (ie point to it) or include it here
\par The main principles for this system are visibility, match with the real world, consistency and error prevention. Where the main focus is on visibility. For the visibility principle, where the focus is on the user knowing the systems state, the design succeeds according to the expert. The user is aware of the actions available to them as the system progresses through its states of processing files. This is from when the system only provides loading actions to the user on startup, to when they are being processed by not providing any functions at that time, to functions relevant to editing the timeline when its produced. The use of red borders in input fields and the disabling of submit buttons when the data inserted is not valid, informs the user of an error in their input, which has to be fixed to proceed. In addition, this design matches the error prevention principle, as it informs the user of input errors, and highlights where the error was made for them to change it. In the case where the user does not know what the correct input is, hints are given. These consist of the date format or by how much they exceeded the allowed character limit for the summary of an event. These are consistently used throughout the system, along with the clearly marked and always to the left "Cancel" buttons to allow the user to not commit their changes. In addition, confirmation dialogs are used whenever costly actions are being performed (e.g. the deletion of an event, subjects for an event or the removal of documents from the timeline). These along with the non-technical language used throughout the project should allow the system to be accessible by any user.
%expert suggestions
\par The expert suggested to improve the visibility in the removal of subjects in the edit dialog, as pressing on the subjects to prompt the delete dialog is not intuitive. In addition, the zooming in function of the range view should be accompanied with an indicator showing the amount of zoom by a percentage value, as this is standard with any zooming in function in other systems (e.g. Adobe Reader, Microsoft Paint, etc.). To aid the user in recognising the actions available to them suggestions were made to include icons for actions (e.g. an upload icon for loading documents). A minor suggestion in the processing files was to change the progress bar, to a horizontal bar that fills up as documents are processed. However, these are not as accurate as one file can require more or less processing power and time than another document, but in the progress bar they would have equal impact.
\par From this evaluation it can be determined that the system fulfilled its aim of producing a system that is visible to its users. Users are informed of the systems progress during the processing of documents, and are assured that the system is not frozen during this time. They are provided with clearly marked exists and cancels, along with indicators of where errors have been made and how these can be solved. It should be noted that while this was one of the main objectives of the system, it is the least important one, as the system could be used to produce event data as an intermediate JSON, which is then used in a 3rd-party graphical representation. Therefore, the graphical representation of the system should be useful and effective for the user that decides to not use a 3rd-party representation. However, these are welcomed in the system.
\section{Efficiency}
\par Efficiency relates to the time it takes for the system to produce an answer. A system is efficient if it produces a result in an appropriate amount of time based on its input.
\par A way to evaluate this is to run the system on different inputs and record the time taken to produce a timeline. However, the time recorded is specific not only to the machine that the system is being run on, but also the other workload the machine has. If the system is ran with many other 3rd party tasks running in the background then the time will be longer than if it is the only process running. In addition, different machines with different hardware parts may be faster or slower depending on the number of cores of the system, and how the workload is scheduled (cite). The Operating System could also affect this (cite). Also, not all different input sizes can be tested, as the document set size can be infinite. The solution is to consider the time complexity of the main algorithms of the system. The time complexity of an algorithm is, the number of operations that need to be carried out for an input of a given size (cite). This allows to determine how well the system scales with larger inputs, by only considering how many operations are performed as a function of the input.
\par Time complexity focuses on the worst case situation, and does not consider low-order operations. For example, if for an input $n$ an algorithm performs $3n+2$ operations, the low-order term $2$ and the co-efficient $3$ can be omitted. This would result in a time complexity $O(n)$ (where $O$ is called big-Oh). In general the highest order-term is taken, and all the rest are omitted. For example, for $4n^2+10n+2$ would have time complexity $O(n^2)$.  The low-order terms are omitted as with a very large input, or as ${n\to\infty}$ the effect of the number of operations of the low-order terms becomes smaller to irrelevant. The co-efficient of the high-order term is omitted, as a machine can be 4 times faster, or 4 times slower, so it does not affect the greately the number of operations performed.
\par To measure the efficiency of the system, the time complexity of the front-end (graphical part) and back-end (logical part) of the system have been computed separately, as the system can be used as a library to produce a list of events to then use in a separate 3rd party graphical representation. The complexity of each algorithm is presented in the table below (see Figure \ref{fig:timecomplexityTable}).

\begin{figure}[h]
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
Algorithm & Information & Time Complexity\\
\hline
\hline
Document Processing &  \begin{tabular}[x]{@{}c@{}}$n$ - number of documents\\$s$ - number of sentences\\$w$ - number of words\end{tabular} & $O(nsw^2)$ \\
\hline
Range Production & $n$ - number of Results & $O(n^2)$\\
\hline
Range Timeline View & $n$ - number of Results & $O(n)$\\
\hline
Traditional Timeline View & $n$ - number of Results & $O(n)$\\
 \hline
\end{tabular}
\end{center}
\caption{Time complexity of main Algorithms in System}
\label{fig:timecomplexityTable}
\end{figure}

\subsection{Back-End}
\subsubsection{Processing Documents}
\par The two main algorithms are the processing of documents, and the production of Ranges. The complexity of each will be presented and discussed. It assumes the complexity of annotating the text in the documents is $O(w)$ (for all $w$ words in the document), and that the documents are annotated before being processed.
\par The algorithm of processing documents is presented below (see Algorithm \ref{alg:documentsProcessing}). For a list of $n$ documents, it processes each. It can process $x$ documents in parallel at a time. Depending on the users setting value $x$ can be 1 or ${x\to\infty}$ ($x$ tends to infinity). If ${x\to\infty}$, then it can be determined that the complexity of the algorithm is given by complexity of processing 1 document (the largest). As all documents are processed in parallel. \par When a document is processed (see algorithm \ref{alg:processDocument}), each sentence is checked for a temporal expression, before a summary or any other processing is done. In the worst case, all sentences $s$ in a document have to be fully processed. In such a case, the date, the subjects, and the summary need to be performed. These are all done after each other, so it can be determined that the computation complexity of processing one sentence is given by $max(getDate, getSubjects, getSummary)$. Where $max$ will return the greates time complexity of the three operations. 
\par To get the date of a sentence, with $w$ words, each temporal expression needs to be processed. The processing of a temporal expression is linear (see the getDate algorithm presented in the Implementation Chapter), as it does not depend on the input directly, it performs less than 3 steps depending on how much the temporal expression can be broken up to. In the worst case, every word in the sentence is a temporal expression (which does not happen normally in a sentence of well-written document, but it can still occur, and the focus is on the worst case). Thereby, the time complexity for getDate is $O(w)$.
\par To select the subjects of a sentence, the NER(footnote) annotator is used to determine which words are of interest. In the worst case, all words can be of interest. The words are not processed when selected, thereby the time complexity of selecting all the subjects in a sentence is $O(w)$.
\par To create the summary of a sentence, with $w$ words, the hedge-trimmer algorithm is used (see the Algorithm presented in the Design Chapter). In the paper, the time complexity of the algorithm is not presented as the algorithm is not explicitly provided, but rather explained. Thereby, the algorithm implemented was analyzed. As a grammatical tree is produced, and each word in the sentence is a leaf. The structure of the tree varies, however it can be assumed that the worst case grammatical tree produced is a full-binary tree. If there are $w$ leaves in the tree (i.e. each word in the sentence is a leaf), then there are $2w-1$ nodes in the tree. The rules of the algorithm are applied one after the other. Thereby, the time complexity is given by the rule with the highest time complexity. The first two rules traverse the tree once (at most), hence have a complexity $O(2w-1) = O(w)$. The last step, where the tree is iteratively shortened, it is done until the tree is below the threshold. In the worst case, the threshold can be 0 (not allowed in the program as the minimum threshold value is 0). Note that the threshold value is the number of leaves. At each step, for the algorithm to continue, at least one node needs to be removed. However, if an inner node is removed then its children (including the leaf is removed, thereby getting closer to the threshold value of 0). At each step the tree needs to be traversed. Thereby, traversing a tree (as presented earlier) has complexity $O(w)$, which is done in the worst case $w$ times (i.e. a word is removed each time, until the word count is 0, and the threshold has been reached).Thereby, the complexity to create a summary is given by $O(w^2)$.
\par Therefore, the processing of a sentence with $w$ words is given by $O(w)+O(w)+O(w^2)$, which can be simplified to $O(w^2)$ (removed low order terms). Assuming that the set of $n$ documents is processed one after the other (i.e. at most 1 document can be produced in paralled), and that all the documents have $s$ sentences (or less). Where each sentence has $w$ words (or less). The running time of processing one document is $O(sw^2)$. Then the running time of processing $n$ documents is $O(nsw^2)$. Where it is not known whether $n$, $s$, or $w$ is the high-order term. The time complexity of annotating a document is omitted as it can be considered that $sw^2 > w$.
\par If the system consists of many documents, with less sentences that do not have many words, then the running time is given by $O(n)$. Which is extremely efficient for processing $n$ documents, as it suggests that the system will increase in processing time linearly with the input size of the number of documents. Thereby, producing a high efficiency as it scales linearly with input.

\subsubsection{Range Production}
\par Ranges are produced with the algorithm presented in the Design Chapter (??). Based on an input of $n$ Results, they have to be sored, added to existing Ranges or create new Ranges, and then sort the Ranges.
\par Sorting Results in Java has a complexity of $O(n\log n)$. This is due to Java using merge-sort when comparing the results (footnote). Merge-sort consits of recursively breaking down the problem space in half (which produces the $\log n$ part), and then building the sequence back up (which produces the $n$). However, it should be noted that with sequences that are almost sorted Java 8+, will have a time complexity of $O(n)$ as it uses TimSort\footnote{\url{https://bugs.openjdk.java.net/browse/JDK-6804124}}. 
\par Each Result needs to be added to the trees. As before it is added to a tree, it is checked whether or not it should be added to that Range tree (by checking  if the dates of the Result and Range overlap), not all trees need to be traversed completely to add the Result. In the worst case, all trees at that point need to be checked and a new tree needs to generated to hold this Result. This can be the case when the dates of events are all completely disjoint.  In such case, the amount of Trees that need to checked increases by 1 for each Result added. Assuming that the time complexity to check whether a Result can be added to a tree is $O(1)$, then the time complexity is given by the sum of 0 to $n$. As in the first step, no Ranges exist so nothing needs to be checked. Then one tree needs to be checked, then two trees, and so on. Where at the $n$th Result, $n-1$ trees need to be checked, each with a complexity $O(1)$. Threby, the total complexity is given by the sum of 0 to $n$, which is given by ${n(n+1)}/2$. This produces a time complexity $O({n^2+n}/2)$, which is $O(n^2)$.
\par After all the trees have been produced, then the Ranges need to be sorted. Where in the worst case there is one full expanded tree. Which is a tree where for each Result it had to be expanded, because the dates overlapped (but where never fully contained within each other). This leads to a full binary tree due to how the trees are expanded by producing a expanded node, with a new node that holds the new Result, and the older subtree on the left. Where the $n$ Results are on the leaves, so there are $2n-1$ nodes. Which using the Java sorting algorithm leads to $O(n\log n)$.
\par Thereby, the time complexity of the Range Production algorithm is given by its greatest complexity of its operation. Which the adding of Results to the tree. Therefore, its time complexity is $O(n^2)$.
\subsection{Front-End}
\par In this section the focus is on the creation of the range and traditional timeline views.
\subsubsection{Range View}
\par As can be seen by the algorithm to produce the Range view (??), it considers each Range separetely when building it. In the worst case, there is one Range with a fully expanded tree (i.e. a full binary Range tree). If the tree holds $n$ results, which are held at the leaves, then it has a total of $2n-1$ nodes. For all nodes it must produce a layout, which includes the production of its children recursively. Therefore, $2n-1$ layouts are produced, so the time complexity is given by $O(n)$. 
\par There can be performance issues, as there are embedded layouts, and many views being created and graphically shown. However, the focus in this section is to consider the time for the system to produce the UI, not how heavy it is for the system's memory.
\subsubsection{Timeline UI}
\par The creation of the timeline view is trivial. For a list of $n$ results, the layout for each row of the listview is produced. Hence, $n$ rows are produced. This leads to a time complexity of $O(n)$.
\par It should be noted that the ListView used in the system does not hold all rows in memory at once. The layout of a row is only produced if it needs to be shown, i.e. that part of the ListView is visible to the user (cite). Hence, the time complexity to produce the timeline view is less than $O(n)$, as the system does not show all the Results. Only when the size of the ListView is smaller than the amount of allocated screen space it is given, will it have to produce $n$ rows, and thereby have a time complexity of $O(n)$.
\section{Effectiveness}
-how did you test?
-participants data?
-safeguard participants
-data set
-why test this way
\par The effectiveness of the system is how well it is at producing timelines. This requires an experimental testing to determine whether the produced timelines are correct. The following section explains the testing process and presents the data.
\subsection{Testing Preparations}
\par In order to test the effectiveness of the timelines, test participants were asked to build manual timelines given by a document. These were compared with the produced timelines to check the percentage of matched events, the percentage of different dates, and the number of events that were not picked.
\par There is no perfect timeline. Each timeline produced by test participants will have differences. The aim is to aggregate them, and compare them to the manual timeline, in order to determine whether the system had the same general sense.
\par Participants were kept anonymous during the entire process, and will be kept anonymous during the presentation of data. The participants came from different academic backgrounds, were of different ages, and had different levels of exposure to the document data sets. A clear limitation in the data gathering was the availability of participants. Therefore, further data could not be gathered. 
\par The data sets consisted of publicly available newspaper articles of different domains: politics, criminal cases, and short bibliographys. To ensure that a substantial amount of data could be gathered, articles that were rich in temporal expressions were picked. This allowed for lower inaccuracies and determining how close the system was in producing similar timelines.
\par Keeping the participants anonymous, and the data sets being publicly available, led to the project not requiring an ethical approval. As well as following the provisions from BCS, in protecting user data and fair use.

\subsection{Testing}
\par Participants were provided the document, and were asked to select events line-by-line. It was important that the events selected came from individual sentences, to be able to compare to the system, which follows the same approach. 
\par There was no time limit for the participants, as the aim was for them to produce correct timelines instead of producing timelines quickly, as the system would be at a clear advantage in that regard.
\par The manual events produced by the participants, included the date of the event and a short summary of what occurred. It would occur that the dates were sometimes ambigious, in which case the participants indentified after/before which events they believed this event occurred. This is an area that the current system lacks, as it cannot establish links between events.
\par The manual timelines were collected and were each compared to the system timeline for the same document. The document data set was of 3 documents.

\subsection{Analysis}
\par The comparison of the manual timelines consisted in: the number of events collected, the percentage of matched and unmatched sentences, the percentage of mismatched exact dates, and the similarity in summaries. 
\par The number of events allows to determine what are the expected amount of events for a given document, and how close the system was in reaching it. This can be combined with the other data to determine if the timeline is to broad in its event generation, or too specific.
\par The matching and unmatching of sentences, consists of identifying which sentences the users used in their generation of events and which the system used (as in the system it holds the original sentence that produced the event). This allows to pinpoint the systems weakest components. If the events do not match the events of the manual timelines, but they come from the same sentences, then it can be determined that the issue is not in the identification of events in the documents but the production of them.
\par Mismatched dates consists of determining how accurate the system is in producing exact dates for events. It is expected that the system will perform in extremely ambigious temporal expressions that can only be linked to other temporal expressions, but cannot be pinpointed to an exact date. For test participants this is not a problem, as they understand context and can apply their own thinking to determine whether an event occurred before or after another event, even when there is no temporal data to support this. This is done by checking system event dates to participant dates for events originated from the same sentence.
\par The similarities in summaries determines how effective the system is in conveying the meaning of the original sentence that produced it. This is done by comparing the system produced timeline with its summary and subjects, and the manual summary, and checking whether they convey the same key idea. This is generally done by checking whether they have the same keywords.
\par The results of the following are presented in the table below (see Figure \ref{fig:tableAnalysis}). The name of the article used is provided, along with its source and its published date.
\begin{figure}[h]
\begin{subfigure}{.5\textwidth}
\begin{tabular}{ |p{5cm}|c|c|c| } 
 \hline
Comparing System to Manual Timelines From... & Participant 1 & Participant 2 & Participant 3\\
\hline
\hline
Difference In collected events (+/-) (19 events collected by system) &  +5 & -1 & -3 \\
\hline
Percentage (\%) of events produced by the same sentence & 66.7 & 83.3 & 87.5\\
\hline
Percentage (\%) of events produced by other sentence & 33.3 & 16.7 & 12.5\\
\hline
Percentage (\%) of mismatched dates & 0.0 & 6.7 & 0.0\\
\hline
Percentage (\%) of similar summaries (matching keywords) & 75.0 & 73.3 & 78.6\\
\hline
Unkown Dates of Manual Timelines From... & 4 & 2 & 2\\
\hline
\end{tabular}
\caption{BBC: US elections 2016: Donald Trump Life Story (Published 20th of January 2017)}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\begin{tabular}{ |p{5cm}|c|c|c| } 
 \hline
Comparing System to Manual Timelines From... & Participant 1 & Participant 2 & Participant 3\\
\hline
\hline
Difference In collected events (+/-) (4 events collected by system) &  +5 & 0 & +3 \\
\hline
Percentage (\%) of events produced by the same sentence & 44.4 & 100.0 & 57.1\\
\hline
Percentage (\%) of events produced by other sentence & 55.6 & 0.0 & 42.9\\
\hline
Percentage (\%) of mismatched dates & 0.0 & 0.0 & 0.0\\
\hline
Percentage (\%) of similar summaries (matching keywords) & 50.0 & 50.0 & 50.0\\
\hline
Unkown Dates of Manual Timelines From... & 0 & 0 & -1\\
\hline
\end{tabular}
\caption{Guardian: British Toddle Abducted, Police believe (Published 5th of May 2007)}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\begin{tabular}{ |p{5cm}|c|c|c| } 
 \hline
Comparing System to Manual Timelines From... & Participant 1 & Participant 2 & Participant 3\\
\hline
\hline
Difference In collected events (+/-) (6 events collected by system) &  +2 & +1 & +1 \\
\hline
Percentage (\%) of events produced by the same sentence & 75.0 & 88.5 & 71.4\\
\hline
Percentage (\%) of events produced by other sentence & 25.0 & 12.5 & 28.6\\
\hline
Percentage (\%) of mismatched dates (on matched events) & 33.3 & 16.7 & 0.0\\
\hline
Percentage (\%) of similar summaries (matching keywords) & 66.7 & 66.7 & 80.0\\
\hline
Unkown Dates of Manual Timelines From... & 1 & 0 & 1\\
\hline
\end{tabular}
\caption{Guardian: Madeleine McCann detectives arrive in Portugal to question 11 suspects (Published 9th of December 2014)}
\end{subfigure}

\caption{Manual vs Automated Timelines Comparison}
\label{fig:tableAnalysis}
\end{figure}
%explain that same sentence means from system to timeline 
%what can be concluded (ie on average summary ...), and final conclusion of system
\par It should be noted that in the vents produced by the same sentence considers the amount of events in the manual timeline that are produced from the same sentences as the events in the automated timeline. This aims to check how many of the manual timeline events are produced from the same text as the automated events.
\par It can be concluded that for this data set the system produced an above ?? certainty in selecting the correct event sentences. The uknown dates were not picked up by the system, as they did not contain temporal expressions, but were related to other events, which the participants detected. It is clear that the production of summaries is not optimal ??. It shows the weaknesses of decision-based headline generation algorithms, and the reason for the growing prominence of statistic based models. An effective point of the system was the production of exact dates from temporal expressions. This value is ignoring the events that were not picked by the system. For the temporal expressions selected by the system, the exact dates produced match the dates given by participants for those events.
\par Therefore, the system does not produce perfect summaries that convey the same meaning as the original sentence. However, the system is effective in the production of exact dates from temporal expressions, and has an over ?? in the identification of events in text.


































