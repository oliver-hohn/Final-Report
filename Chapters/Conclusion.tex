\chapter{Conclusion and Future Work}

\section{Conclusion of Project}
\par In conclusion, the project aimed to build a system that extracts events autonomously from documents, and then produces a timeline with the events. The Stanford CoreNLP tool was the major library used to categories sets of words into predefined categories (NER annotator), and build the grammatical trees (POS annotator). A trimming algorithm was used to produce summaries of sentences through headline generation. The use of multi-threading allowed for parallel processing of documents, and thus faster times to produce the event data and populate the timelines. 

\par Open-source will allow the project to be further developed, and included in other event extraction systems.  Since the code produced is fully documented, and the back-end of the system is separate of its graphical counterpart, the system can be integrated as a library in other projects. The intermediate produced JSON allows the Results of the system to be used in third-party applications. Using Gradle, and a produced wrapper, allows for portability to be achieved, as the required libraries are retrieved before run-time, and the system can be ran with one command\footnote{gradlew run}. 

\par Issues occurred, and new requirements appeared during the production of the system. These include the creation of exact dates from normalized dates, wrong input documents, and the encapsulation of events for a new timeline view. These issues were identified and appropriately solved. Of course, as with any project, the initial design was changed, but through Agile methodology this could be tackled and did not hinder the implementation of the system. Changes include minor alterations in the systems architecture to allow for the independence between the logic of the system and its graphical representation.

\par The use of running time complexity allowed for the applications efficiency to be evaluated independently of the system it is run on, and on the data size given as input. Using an expert heuristic allowed the produced UI to be evaluated for visibility. Since the heuristic is a standard, it has been thoroughly tested before-hand by others. From the evaluation of the effectiveness of the system, it can be determined that the system is effective in identifying the events in the documents used (that consisted of different domains). However, the summary module of the system requires further work. It is not possible to say with certainty how effective the system is in general, as not enough data is available for such analysis. However, with more time this can be done.

\section{Future Work}
\par Future works consist of areas of possible development that would improve the overall effectiveness of the system. The areas are described below. Implementing any of these would be an improvement, however the challenge is in being able to combine them all to produce a system that can be used at a commercial level, and would be unrivalled.

\par \textbf{Neural Net} - As mentioned in the Background Chapter, there is a heavy use of Neural Nets for text summarization, as can be seen from the works of \cite{chopraaulirush2016} and \cite{rushchopraweston2015}. These are often combined with noisy-channel models that use data sets for statistical computation of summaries. The main benefit of such a system would be to provide better summaries. The reason for them not being used in the project is the large data set required and the amount of computation done to produce a summary of one sentence.

\par \textbf{Machine Learning} - Machine learning allows an algorithm to produce accurate results by using a data set \cite{machinelearning}. In this system, it could be used in the production of events. For example, when a user edits an event it can be assumed that they produced a corrected event. This data can then be used and considered in the creation of other events by the system, to produce more precise ones.

\par \textbf{Cloud} - A cloud allows a service to be provided independent of its software and hardware \cite{cloud}. This is done by deploying the system on a remote server, and giving clients an interface to interact with it. Thereby it is not required for the client to have a powerful machine, and it does not affect their systems performance. The downfall is the cost. However, the performance could be improved as specific hardware could be used to improve the efficiency and it could be combined with Machine-Learning and Neural Nets to allow the system to improve in its event identification and production.

\par \textbf{Extending the Stanford CoreNLP tool} - An issue in the tool used, is that it does not attempt to link ambiguous temporal expressions, i.e. it may be known that an event occurred before another. For example, a person must be born first before they can work (example taken from \cite{mccloskymanning2012}). In the system when an exact date cannot be given, an ambiguous date is produced. For example, for both born and working it could produce a "PAST\_REF". This would then cause the system to assign both events to the same range of dates, i.e. 0001-01-01 up to the reference point used for the document. However, these can be made more precisely.  Since a person must be born first, and be over the age of 16 to work. Thereby the range of dates for the work event could be identified more precisely. The problem arises, from the tool and the produced system considering each sentence independently of each other. It would be beneficial if the events were to be linked one after the other, such that it may not be known when someone was born or when they worked, but that the event of them working is after the event of them being born. This would require building on the NLP tool used in the system.