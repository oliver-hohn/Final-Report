\chapter{Implementation}
-How implemented
\section{Approach}
\par The development approach taken focused on the business-logic, or back-end, of the system. From the two main possible development methodologies, Waterfall and Agile, the latter was used. In Waterfall, the devleopment process is a sequential process, where the dvelopment is considered as a sequence of phases that are completed one after the other (cite). In Agile, the focus in adaptive planning, evolutionary development, and continous improvement. The advantage of using an Agile approach over a Waterfall approach is that new features can be mplemented into the system easier. 
\par The most used way of using Agile methodology is through Sprint cycles. These are short development cycles, where a set of features must be implemented, alongside their tests to ensure the correctness of features(discussed in the Testing section). For this project, the cycles combined with the supervisor meetings. Since in them, the new features that were implemented were discussed alongside the new features that were to be implemented in the next cycle. 
\par A clear example of the advantage of this system was when the a new timeline view was suggested. In this new view, rather than having the events row by row, events that occurred during the same time period should be grouped. In addition, events that happened within that time period should be encapsulated by the larger events. This could be implemented in the system, due to the separation of the business logic and the view, and the development approach used. In a Waterfall model, the development is more structured, and thereby it is extremely useful for static requirements, i.e. requirements that will not change. However, in this case it would have caused issues in implementing the new view as it would require going up the Waterfall if the view of the system had already been implemented, or waiting until that step of the waterfall had been reached.
\par While the Agile methodology is mostly used in software development teams, it can be applied to single development projects. Since the structure allows for reviews of features which can be matched with supervisor meetings, and changes in the requirements of the project.
\section{Tools \& Software Libraries}
-development environment
-why used that environment
-software libraries (include an example use)
-why
\par The development environment of the project is a 64-bit Windows 10 machine, with a Intel Core i7-6700HQ CPU at 2.60GHz and 16.0GB of Random Access Memory (RAM). It includes a Java Intelligent Development Environment (IDE), with Git for version-control, and Gradle for dependency management. 
\par The use of verion-control allows development of features separata of working code, and only adding them to the working version if the required tests pass. It should be noted that Git flow was used. This involves having a develop branch with the newest working features of the system. The master branch will only contain the lastest fully implemented working version of the product. This allows for mistakes in development to be rolled-back to a state where the system worked correctly.
\par Gradle allows for libraries to be regarded as dependencies of the project. Such that when the system is ran on a separate machine, it will retrieve all the missing libaries used in the project before compiling and running the program. This allows for the system to be shared to other users, without having to include the libraries with the distribution of the code, as the required libraries and the version will be downloaded to the users system when they run the command: \textbf{gradlew run}. Where the gradlew is a wrapper for gradle, such that the user does not even have to have Gradle in their system to launch the system. This provides obvious advantages of portability and general use.
\par As mentioned in the Background chapter, multiple libraries exist to aid the task of Natural Language Processing. These are especially needed for the Normalized Entity Recognition(NER) and Text Summary task. As an NER annotator will tag certain words, or collection of words into predefined categories such as People, Companies, Locations, and Money. This is extremely useful for the task of identifying dates in sentences, and the subjects described in the sentences. In addition, these tools can aid in the tagging of words using the P.O.S Treebank, which is required for the implementation of the Hedge-Trimmer algorithm \cite{dorrzajicschwartz2003} for headline generation (i.e. summary of a sentence). The main advantage of using libraries for this task over developing these annotators, is that building such an annotator requires multiple developers and many years of work. This can be seen from the 	release history of the StanfordCoreNLP tool which intially released in 2010, but still in October 2016 new releases have been made\footnote{\url{http://stanfordnlp.github.io/CoreNLP/history.html}}. The main two NLP tools are Apache's OpenNLP\footnote{\url{https://opennlp.apache.org/}} and Stanford's CoreNLP\footnote{\url{http://stanfordnlp.github.io/CoreNLP/index.html}}. For this project the Stanford's tool was used in the implementation, as it provides an extended documentation and examples of using their tools, along with specific sections for each of their annotators. The Stanford tool is the main library used throughout the project, as the project is reliant on its NER and POS annotators (cite the stanford annotators). It comes with models, that are loaded during the initialisation of the system. These models are used in the annotators to determine whether certain words fall in predefined categories, or which POS tag should be given to them, through the use of statistics that are based on the models. 
\par As the two main NLP libraries available are Java implementations, the decision was made to build the system in that language. It would be problematic to build the system in a different language to its libraries, as it would require to make the two programming languages communicate with each other, which can cause unpredictable problems in the development and execution of the system.
\par Additional libraries in the development include JUnit for Unit testing. This allows for features of the system to be developed and for them to be tested for correctness. With the addition of the Git flow, when new features are developed, they are done on a separate branch. Before they are joined to the latest working version of the system the tests for other features and the current developed feature can be ran, thereby ensuring that the system is still working as expected even with the new feature. Unit testing allows for part of a system to be ran, and then to produce a result and compare it to an expected result. The test would then pass if the results match. Testing will be further discussed in one of the following sections.
\par Libraries for text extraction of .pdf and .docx file types are required, as the enconding of these files is not in a plain text model. Therefore the Apache POI\footnote{\url{https://poi.apache.org/}} and the Apache PDFBox\footnote{\url{https://pdfbox.apache.org/}} are used. In addition to text extraction, the PDFBox library along with the Apache Commons library allows the creation of PDFs (with text wrapping), which is required to save the timeline as a PDF. The Google GSON\footnote{\url{https://github.com/google/gson}} library is used for the creation of JSONs, which is required to produce an intermediate JSON of the timeline. The RichTextFX\footnote{\url{https://github.com/TomasMikula/RichTextFX}} library along with JavaFX are used to build the graphical interface of the system. All of the libraries included are provided with licenses that allow its use in systems, along as the system is made publicly available, which will be done as the resulting system will be open-source.

\section{Issues}
-ner dates (explain, then present how to solve it through examples of code)
-new timeline view (present how to solve it through examples of code)
\section{Testing}
\section{UI}
\section{Algorithms}
-main algorithms: for building events, building ranges, pdf save, json save, processing files
